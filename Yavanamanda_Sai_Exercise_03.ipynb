{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sainikhila11/SaiNikhila_INFO5731_Spring2024/blob/main/Yavanamanda_Sai_Exercise_03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRwkJBn70nX"
      },
      "source": [
        "# **INFO5731 In-class Exercise 3**\n",
        "\n",
        "The purpose of this exercise is to explore various aspects of text analysis, including feature extraction, feature selection, and text similarity ranking.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARqm7u6B70ne"
      },
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting text classification or text mining task and explain what kind of features might be useful for you to build the machine learning model. List your features and explain why these features might be helpful. You need to list at least five different types of features."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I would like to consider Sentiment Analysis on Social Media Comments\n",
        "In this text classification task, here primarity i want to differentiate and\n",
        "classify social media comments into positive, negative, or neutral sentiments.\n",
        "\n",
        "Features:\n",
        "\n",
        "TF-IDF (Term Frequency-Inverse Document Frequency): TF-IDF can capture the importance of words in a comment relative to their frequency across all comments. Specific words may strongly indicate positive or negative sentiment.\n",
        "\n",
        "Sentiment Lexicons:bUsing sentiment lexicons or dictionaries that assign sentiment scores to words can help capture the overall sentiment expressed in the comment.\n",
        "\n",
        "Emoticon Count: Counting the occurrences of emoticons (e.g., :) or :( ) can provide additional information about the sentiment expressed in a comment.\n",
        "\n",
        "Number of Positive and Negative Words: Counting the number of positive and negative words in a comment can provide a quantitative measure of sentiment.\n",
        "\n",
        "Readability Scores (Flesch-Kincaid Grade Level): Different sentiment levels may be associated with different readability levels. Simpler language may be indicative of a more positive sentiment.\n"
      ],
      "metadata": {
        "id": "f3dNF8jjazLy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUjBE6C70nf"
      },
      "source": [
        "## Question 2 (10 Points)\n",
        "Write python code to extract these features you discussed above. You can collect a few sample text data for the feature extraction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "EoQX5s4O70nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbd43651-8ae6-4099-be38-6e891a5371e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting textstat\n",
            "  Downloading textstat-0.7.3-py3-none-any.whl (105 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.1/105.1 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyphen (from textstat)\n",
            "  Downloading pyphen-0.14.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyphen, textstat\n",
            "Successfully installed pyphen-0.14.0 textstat-0.7.3\n",
            "                                 Text  \\\n",
            "0  I love this product! It's amazing.   \n",
            "1        This is terrible. I hate it.   \n",
            "2               Neutral comment here.   \n",
            "3           Great news! I'm so happy!   \n",
            "4  Not impressed with the service. :(   \n",
            "\n",
            "                                              TF-IDF  Sentiment Score  \\\n",
            "0  [0.4821401170833009, 0.0, 0.0, 0.0, 0.0, 0.0, ...           0.6125   \n",
            "1  [0.0, 0.0, 0.0, 0.0, 0.4821401170833009, 0.0, ...          -0.9000   \n",
            "2  [0.0, 0.5773502691896258, 0.0, 0.0, 0.0, 0.577...           0.0000   \n",
            "3  [0.0, 0.0, 0.5, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, ...           1.0000   \n",
            "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4472135954999...          -0.6250   \n",
            "\n",
            "   Emoticon Count  Positive Word Count  Negative Word Count  Readability Score  \n",
            "0               0                    2                    0                2.1  \n",
            "1               0                    0                    2                0.9  \n",
            "2               0                    0                    0                5.6  \n",
            "3               0                    1                    0                0.5  \n",
            "4               1                    0                    0                2.9  \n"
          ]
        }
      ],
      "source": [
        "# Install required modules\n",
        "!pip install textstat\n",
        "\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from textblob import TextBlob\n",
        "import re  # Import regular expression module\n",
        "import textstat  # Import the required module\n",
        "import pandas as pd\n",
        "\n",
        "# Sample data\n",
        "data = [\"I love this product! It's amazing.\",\n",
        "        \"This is terrible. I hate it.\",\n",
        "        \"Neutral comment here.\",\n",
        "        \"Great news! I'm so happy!\",\n",
        "        \"Not impressed with the service. :(\"]\n",
        "\n",
        "# Feature 1: TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(data)\n",
        "\n",
        "# Feature 2: Sentiment Lexicons\n",
        "def get_sentiment_score(text):\n",
        "    blob = TextBlob(text)\n",
        "    return blob.sentiment.polarity\n",
        "\n",
        "# Feature 3: Emoticon Count\n",
        "def get_emoticon_count(text):\n",
        "    # Using regular expression to find emoticons\n",
        "    emoticons = re.findall(r'(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
        "    return len(emoticons)\n",
        "\n",
        "# Feature 4: Number of Positive and Negative Words\n",
        "def get_positive_negative_word_count(text):\n",
        "    positive_words = [\"love\", \"happy\", \"great\", \"amazing\"]\n",
        "    negative_words = [\"terrible\", \"hate\", \"not impressed\"]\n",
        "\n",
        "    positive_count = sum(1 for word in positive_words if word in text)\n",
        "    negative_count = sum(1 for word in negative_words if word in text)\n",
        "\n",
        "    return positive_count, negative_count\n",
        "\n",
        "# Feature 5: Readability Scores (Flesch-Kincaid Grade Level)\n",
        "readability_scores = [textstat.flesch_kincaid_grade(comment) for comment in data]\n",
        "\n",
        "# Apply features to sample data\n",
        "df = pd.DataFrame(data, columns=['Text'])\n",
        "df['TF-IDF'] = list(tfidf_matrix.toarray())\n",
        "df['Sentiment Score'] = df['Text'].apply(get_sentiment_score)\n",
        "df['Emoticon Count'] = df['Text'].apply(get_emoticon_count)\n",
        "df['Positive Word Count'], df['Negative Word Count'] = zip(*df['Text'].apply(get_positive_negative_word_count))\n",
        "df['Readability Score'] = readability_scores\n",
        "\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oSK4soH70nf"
      },
      "source": [
        "## Question 3 (10 points):\n",
        "Use any of the feature selection methods mentioned in this paper \"Deng, X., Li, Y., Weng, J., & Zhang, J. (2019). Feature selection for text classification: A review. Multimedia Tools & Applications, 78(3).\"\n",
        "\n",
        "Select the most important features you extracted above, rank the features based on their importance in the descending order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "2CRuXfV570ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7de557e1-6cdb-474a-e152-3c1978391588"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "               Feature  Mutual Information Score\n",
            "3  Negative Word Count                  0.316667\n",
            "2  Positive Word Count                  0.283333\n",
            "0      Sentiment Score                  0.216667\n",
            "1       Emoticon Count                  0.000000\n",
            "4    Readability Score                  0.000000\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_selection import mutual_info_classif\n",
        "\n",
        "# Extract the selected features\n",
        "selected_features = ['Sentiment Score', 'Emoticon Count', 'Positive Word Count', 'Negative Word Count', 'Readability Score']\n",
        "X_selected = df[selected_features].copy()\n",
        "\n",
        "# Define target variable\n",
        "y = [1, 0, 1, 1, 0]  # Assuming 1 for positive sentiment, 0 for negative sentiment\n",
        "\n",
        "# Compute mutual information scores\n",
        "mutual_info_scores_selected = mutual_info_classif(X_selected, y)\n",
        "\n",
        "# Create a DataFrame to display feature importance\n",
        "feature_importance_selected_df = pd.DataFrame({'Feature': selected_features, 'Mutual Information Score': mutual_info_scores_selected})\n",
        "feature_importance_selected_df = feature_importance_selected_df.sort_values(by='Mutual Information Score', ascending=False)\n",
        "\n",
        "# Display ranked features\n",
        "print(feature_importance_selected_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nZGAOwl70ng"
      },
      "source": [
        "## Question 4 (10 points):\n",
        "Write python code to rank the text based on text similarity. Based on the text data you used for question 2, design a query to match the most relevant docments. Please use the BERT model to represent both your query and the text data, then calculate the cosine similarity between the query and each text in your data. Rank the similary with descending order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "b4HoWK-i70ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3dbf00bf-b0b8-4737-d7f9-351432b5d188"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (2.5.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.32.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.38.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.1.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.20.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (23.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.4.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.3.0)\n",
            "Ranked texts based on cosine similarity:\n",
            "1. Similarity: 0.6312, Text: I love this product! It's amazing.\n",
            "2. Similarity: 0.2657, Text: Great news! I'm so happy!\n",
            "3. Similarity: 0.2208, Text: Neutral comment here.\n",
            "4. Similarity: 0.0310, Text: Not impressed with the service. :(\n",
            "5. Similarity: -0.0707, Text: This is terrible. I hate it.\n"
          ]
        }
      ],
      "source": [
        "# Install required libraries\n",
        "!pip install sentence-transformers\n",
        "!pip install scikit-learn\n",
        "\n",
        "# Import libraries\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Sample data\n",
        "data = [\n",
        "    \"I love this product! It's amazing.\",\n",
        "    \"This is terrible. I hate it.\",\n",
        "    \"Neutral comment here.\",\n",
        "    \"Great news! I'm so happy!\",\n",
        "    \"Not impressed with the service. :(\"\n",
        "]\n",
        "\n",
        "query = \"I'm looking for a great product with positive reviews.\"\n",
        "\n",
        "# Load pre-trained BERT model\n",
        "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "\n",
        "# Encode the query and text data\n",
        "query_embedding = model.encode([query], convert_to_tensor=True)\n",
        "data_embeddings = model.encode(data, convert_to_tensor=True)\n",
        "\n",
        "# Calculate cosine similarity between the query and each text\n",
        "cosine_similarities = cosine_similarity(query_embedding, data_embeddings)\n",
        "\n",
        "# Create a list of tuples containing the index and cosine similarity\n",
        "similarity_ranking = [(i, cosine_similarities[0][i]) for i in range(len(data))]\n",
        "\n",
        "# Sort the list based on cosine similarity in descending order\n",
        "similarity_ranking.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Display the ranked texts and their cosine similarities\n",
        "print(\"Ranked texts based on cosine similarity:\")\n",
        "for rank, (index, similarity) in enumerate(similarity_ranking):\n",
        "    print(f\"{rank + 1}. Similarity: {similarity:.4f}, Text: {data[index]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "VEs-OoDEhTW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on this exercise**\n",
        "\n",
        "Please provide your thoughts and feedback on the exercises you completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on extracting features from text data. What were the key concepts or techniques you found most beneficial in understanding the process?\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in completing this exercise?\n",
        "\n",
        "Relevance to Your Field of Study: How does this exercise relate to the field of NLP?\n",
        "\n",
        "**(Your submission will not be graded if this question is left unanswered)**\n",
        "\n"
      ],
      "metadata": {
        "id": "IUKC7suYhVl0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The exercise provided a practical and hands-on experience to understand and to apply feature extraction techniques. The instructions were clear, and the provided code snippets were helpful in implementing the feature extraction methods. It was a smooth process overall but looking up and understanding the article mentioned has taken up some time. This exercise taught me how computers understand feelings in text. I focused on sentiment analysis in Natural Language Processing (NLP). I learned to pick out important clues like unique words, sentiment-loaded words, and emoticons. These clues help the computer recognize patterns in language, useful for tasks like sorting positive and negative comments on social media. It's like giving the computer tools to understand human language better."
      ],
      "metadata": {
        "id": "xckPtgteVpUu"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}