{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sainikhila11/SaiNikhila_INFO5731_Spring2024/blob/main/SaiNikhila_INFO5731_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJbEtfqkBXN7",
        "outputId": "352bd4f1-7e1d-4501-a0d5-fa5940b2cf7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai==0.28\n",
            "  Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.9.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2024.2.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.28.0\n"
          ]
        }
      ],
      "source": [
        "pip install openai==0.28"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eftizj9AI9tD",
        "outputId": "5902f1bb-1c63-4daa-c6e8-5c8fecefba94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Definition for Correctness: Correctness is measured by the semantic similarity between the provided disease description and the suggested disease term. Higher similarity indicates higher correctness.\n",
            "Correctness Score: 0.96\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import openai\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "# Set your OpenAI API key\n",
        "openai.api_key = 'sk-proj-9hKjQtT6e6orqiwwd10jT3BlbkFJkW2KlXLZ4Ir2B7s6ja5V'\n",
        "\n",
        "# Define the fixed definition for correctness\n",
        "CORRECTNESS_DEFINITION = \"Correctness is measured by the semantic similarity between the provided disease description and the suggested disease term. Higher similarity indicates higher correctness.\"\n",
        "\n",
        "# Load your dataset\n",
        "df = pd.read_excel('/content/Train0.xlsx')\n",
        "\n",
        "# Define a function to get embeddings for a list of texts\n",
        "def get_embeddings(texts, engine=\"text-embedding-ada-002\"):\n",
        "    try:\n",
        "        response = openai.Embedding.create(input=texts, engine=engine)\n",
        "        embeddings = [item['embedding'] for item in response['data']]\n",
        "        return embeddings\n",
        "    except Exception as e:\n",
        "        print(f\"Error getting embeddings: {e}\")\n",
        "        return [None] * len(texts)\n",
        "\n",
        "# Define a function to perform a preliminary context check\n",
        "def preliminary_context_check(description, disease):\n",
        "    try:\n",
        "        response = openai.Completion.create(\n",
        "            model=\"gpt-3.5-turbo-instruct\",\n",
        "            prompt=f\"{CORRECTNESS_DEFINITION}\\n\\nIs the term '{disease}' contextually relevant to the description '{description}'? If not, suggest the most appropriate term.\",\n",
        "            max_tokens=50,\n",
        "            temperature=0.3\n",
        "        )\n",
        "        corrected_term = response.choices[0].text.strip()\n",
        "        return corrected_term\n",
        "    except Exception as e:\n",
        "        print(f\"Error querying OpenAI for preliminary context: {e}\")\n",
        "        return \"Error\"\n",
        "\n",
        "# Apply the validation function to the dataset\n",
        "correctness_values = []\n",
        "for _, row in df.iterrows():\n",
        "    corrected_term = preliminary_context_check(row['Description'], row['Disease'])\n",
        "    description_embedding = get_embeddings([row['Description']])\n",
        "    disease_embedding = get_embeddings([row['Disease']])\n",
        "    description_emb = description_embedding[0]\n",
        "    disease_emb = disease_embedding[0]\n",
        "    if description_emb is not None and disease_emb is not None:\n",
        "        similarity = 1 - cosine(description_emb, disease_emb)\n",
        "        correctness_values.append(similarity)\n",
        "    else:\n",
        "        correctness_values.append(None)\n",
        "\n",
        "# Calculate Correctness Score\n",
        "correctness_score = sum(correctness_values) / len(correctness_values)\n",
        "\n",
        "# Print Correctness Score and the definition\n",
        "print(f\"Definition for Correctness: {CORRECTNESS_DEFINITION}\")\n",
        "print(f\"Correctness Score: {round(correctness_score, 2)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9MSuyIsDZ4HA",
        "outputId": "65e1f9bd-4249-47e4-ae60-188c750e9d43"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing: 100%|██████████| 4817/4817 [1:10:13<00:00,  1.14it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Definition for Correctness: Correctness is measured by the semantic similarity between the provided disease description and the suggested disease term. Higher similarity indicates higher correctness.\n",
            "Correctness Score: 0.96\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import openai\n",
        "from scipy.spatial.distance import cosine\n",
        "from tqdm import tqdm  # Import tqdm for progress tracking\n",
        "\n",
        "# Set your OpenAI API key\n",
        "openai.api_key = 'sk-proj-9hKjQtT6e6orqiwwd10jT3BlbkFJkW2KlXLZ4Ir2B7s6ja5V'\n",
        "\n",
        "# Define the fixed definition for correctness\n",
        "CORRECTNESS_DEFINITION = \"Correctness is measured by the semantic similarity between the provided disease description and the suggested disease term. Higher similarity indicates higher correctness.\"\n",
        "\n",
        "# Load your dataset\n",
        "df = pd.read_excel('/content/Train1.xlsx')\n",
        "\n",
        "# Define a function to get embeddings for a list of texts\n",
        "def get_embeddings(texts, engine=\"text-embedding-ada-002\"):\n",
        "    try:\n",
        "        response = openai.Embedding.create(input=texts, engine=engine)\n",
        "        embeddings = [item['embedding'] for item in response['data']]\n",
        "        return embeddings\n",
        "    except Exception as e:\n",
        "        print(f\"Error getting embeddings: {e}\")\n",
        "        return [None] * len(texts)\n",
        "\n",
        "# Define a function to perform a preliminary context check\n",
        "def preliminary_context_check(description, disease):\n",
        "    try:\n",
        "        response = openai.Completion.create(\n",
        "            model=\"gpt-3.5-turbo-instruct\",\n",
        "            prompt=f\"{CORRECTNESS_DEFINITION}\\n\\nIs the term '{disease}' contextually relevant to the description '{description}'? If not, suggest the most appropriate term.\",\n",
        "            max_tokens=50,\n",
        "            temperature=0.3\n",
        "        )\n",
        "        corrected_term = response.choices[0].text.strip()\n",
        "        return corrected_term\n",
        "    except Exception as e:\n",
        "        print(f\"Error querying OpenAI for preliminary context: {e}\")\n",
        "        return \"Error\"\n",
        "\n",
        "# Apply the validation function to the dataset\n",
        "correctness_values = []\n",
        "total_iterations = len(df)  # Get total number of iterations\n",
        "with tqdm(total=total_iterations, desc=\"Processing\") as pbar:  # Initialize progress bar\n",
        "    for _, row in df.iterrows():\n",
        "        corrected_term = preliminary_context_check(row['Description'], row['Disease'])\n",
        "        description_embedding = get_embeddings([row['Description']])\n",
        "        disease_embedding = get_embeddings([row['Disease']])\n",
        "        description_emb = description_embedding[0]\n",
        "        disease_emb = disease_embedding[0]\n",
        "        if description_emb is not None and disease_emb is not None:\n",
        "            similarity = 1 - cosine(description_emb, disease_emb)\n",
        "            correctness_values.append(similarity)\n",
        "        else:\n",
        "            correctness_values.append(None)\n",
        "        pbar.update(1)  # Update progress bar\n",
        "\n",
        "# Calculate Correctness Score\n",
        "correctness_score = sum(correctness_values) / len(correctness_values)\n",
        "\n",
        "# Print Correctness Score and the definition\n",
        "print(f\"Definition for Correctness: {CORRECTNESS_DEFINITION}\")\n",
        "print(f\"Correctness Score: {round(correctness_score, 2)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5Jps0A_q6QJ",
        "outputId": "a8233d2c-db62-43dc-c5a2-03d25574df23"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing: 100%|██████████| 4791/4791 [1:43:05<00:00,  1.29s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Definition for Correctness: Correctness is measured by the semantic similarity between the provided disease description and the suggested disease term. Higher similarity indicates higher correctness.\n",
            "Correctness Score: 0.96\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import openai\n",
        "from scipy.spatial.distance import cosine\n",
        "from tqdm import tqdm  # Import tqdm for progress tracking\n",
        "\n",
        "# Set your OpenAI API key\n",
        "openai.api_key = 'sk-proj-9hKjQtT6e6orqiwwd10jT3BlbkFJkW2KlXLZ4Ir2B7s6ja5V'\n",
        "\n",
        "# Define the fixed definition for correctness\n",
        "CORRECTNESS_DEFINITION = \"Correctness is measured by the semantic similarity between the provided disease description and the suggested disease term. Higher similarity indicates higher correctness.\"\n",
        "\n",
        "# Load your dataset\n",
        "df = pd.read_excel('/content/Train2.xlsx')\n",
        "\n",
        "# Define a function to get embeddings for a list of texts\n",
        "def get_embeddings(texts, engine=\"text-embedding-ada-002\"):\n",
        "    try:\n",
        "        response = openai.Embedding.create(input=texts, engine=engine)\n",
        "        embeddings = [item['embedding'] for item in response['data']]\n",
        "        return embeddings\n",
        "    except Exception as e:\n",
        "        print(f\"Error getting embeddings: {e}\")\n",
        "        return [None] * len(texts)\n",
        "\n",
        "# Define a function to perform a preliminary context check\n",
        "def preliminary_context_check(description, disease):\n",
        "    try:\n",
        "        response = openai.Completion.create(\n",
        "            model=\"gpt-3.5-turbo-instruct\",\n",
        "            prompt=f\"{CORRECTNESS_DEFINITION}\\n\\nIs the term '{disease}' contextually relevant to the description '{description}'? If not, suggest the most appropriate term.\",\n",
        "            max_tokens=50,\n",
        "            temperature=0.3\n",
        "        )\n",
        "        corrected_term = response.choices[0].text.strip()\n",
        "        return corrected_term\n",
        "    except Exception as e:\n",
        "        print(f\"Error querying OpenAI for preliminary context: {e}\")\n",
        "        return \"Error\"\n",
        "\n",
        "# Apply the validation function to the dataset\n",
        "correctness_values = []\n",
        "total_iterations = len(df)  # Get total number of iterations\n",
        "with tqdm(total=total_iterations, desc=\"Processing\") as pbar:  # Initialize progress bar\n",
        "    for _, row in df.iterrows():\n",
        "        corrected_term = preliminary_context_check(row['Description'], row['Disease'])\n",
        "        description_embedding = get_embeddings([row['Description']])\n",
        "        disease_embedding = get_embeddings([row['Disease']])\n",
        "        description_emb = description_embedding[0]\n",
        "        disease_emb = disease_embedding[0]\n",
        "        if description_emb is not None and disease_emb is not None:\n",
        "            similarity = 1 - cosine(description_emb, disease_emb)\n",
        "            correctness_values.append(similarity)\n",
        "        else:\n",
        "            correctness_values.append(None)\n",
        "        pbar.update(1)  # Update progress bar\n",
        "\n",
        "# Calculate Correctness Score\n",
        "correctness_score = sum(correctness_values) / len(correctness_values)\n",
        "\n",
        "# Print Correctness Score and the definition\n",
        "print(f\"Definition for Correctness: {CORRECTNESS_DEFINITION}\")\n",
        "print(f\"Correctness Score: {round(correctness_score, 2)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZ3i46fE5PaI",
        "outputId": "44682276-28ab-46d6-8971-2ef0c46ad31b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing: 100%|██████████| 4812/4812 [1:08:14<00:00,  1.18it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Definition for Correctness: Correctness is measured by the semantic similarity between the provided disease description and the suggested disease term. Higher similarity indicates higher correctness.\n",
            "Correctness Score: 0.9641\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import openai\n",
        "from scipy.spatial.distance import cosine\n",
        "from tqdm import tqdm  # Import tqdm for progress tracking\n",
        "\n",
        "# Set your OpenAI API key\n",
        "openai.api_key = 'sk-proj-9hKjQtT6e6orqiwwd10jT3BlbkFJkW2KlXLZ4Ir2B7s6ja5V'\n",
        "\n",
        "# Define the fixed definition for correctness\n",
        "CORRECTNESS_DEFINITION = \"Correctness is measured by the semantic similarity between the provided disease description and the suggested disease term. Higher similarity indicates higher correctness.\"\n",
        "\n",
        "# Load your dataset\n",
        "df = pd.read_excel('/content/Train3.xlsx')\n",
        "\n",
        "# Define a function to get embeddings for a list of texts\n",
        "def get_embeddings(texts, engine=\"text-embedding-ada-002\"):\n",
        "    try:\n",
        "        response = openai.Embedding.create(input=texts, engine=engine)\n",
        "        embeddings = [item['embedding'] for item in response['data']]\n",
        "        return embeddings\n",
        "    except Exception as e:\n",
        "        print(f\"Error getting embeddings: {e}\")\n",
        "        return [None] * len(texts)\n",
        "\n",
        "# Define a function to perform a preliminary context check\n",
        "def preliminary_context_check(description, disease):\n",
        "    try:\n",
        "        response = openai.Completion.create(\n",
        "            model=\"gpt-3.5-turbo-instruct\",\n",
        "            prompt=f\"{CORRECTNESS_DEFINITION}\\n\\nIs the term '{disease}' contextually relevant to the description '{description}'? If not, suggest the most appropriate term.\",\n",
        "            max_tokens=50,\n",
        "            temperature=0.3\n",
        "        )\n",
        "        corrected_term = response.choices[0].text.strip()\n",
        "        return corrected_term\n",
        "    except Exception as e:\n",
        "        print(f\"Error querying OpenAI for preliminary context: {e}\")\n",
        "        return \"Error\"\n",
        "\n",
        "# Apply the validation function to the dataset\n",
        "correctness_values = []\n",
        "total_iterations = len(df)  # Get total number of iterations\n",
        "with tqdm(total=total_iterations, desc=\"Processing\") as pbar:  # Initialize progress bar\n",
        "    for _, row in df.iterrows():\n",
        "        corrected_term = preliminary_context_check(row['Description'], row['Disease'])\n",
        "        description_embedding = get_embeddings([row['Description']])\n",
        "        disease_embedding = get_embeddings([row['Disease']])\n",
        "        description_emb = description_embedding[0]\n",
        "        disease_emb = disease_embedding[0]\n",
        "        if description_emb is not None and disease_emb is not None:\n",
        "            similarity = 1 - cosine(description_emb, disease_emb)\n",
        "            correctness_values.append(similarity)\n",
        "        else:\n",
        "            correctness_values.append(None)\n",
        "        pbar.update(1)  # Update progress bar\n",
        "\n",
        "# Calculate Correctness Score\n",
        "correctness_score = sum(correctness_values) / len(correctness_values)\n",
        "\n",
        "# Print Correctness Score and the definition\n",
        "print(f\"Definition for Correctness: {CORRECTNESS_DEFINITION}\")\n",
        "print(f\"Correctness Score: {round(correctness_score, 4)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mll4rwPcKP8t",
        "outputId": "ae96d852-439f-429b-d8bf-f04cc0250061"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing: 100%|██████████| 4811/4811 [1:07:15<00:00,  1.19it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Definition for Correctness: Correctness is measured by the semantic similarity between the provided disease description and the suggested disease term. Higher similarity indicates higher correctness.\n",
            "Correctness Score: 0.9638\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import openai\n",
        "from scipy.spatial.distance import cosine\n",
        "from tqdm import tqdm  # Import tqdm for progress tracking\n",
        "\n",
        "# Set your OpenAI API key\n",
        "openai.api_key = 'sk-proj-9hKjQtT6e6orqiwwd10jT3BlbkFJkW2KlXLZ4Ir2B7s6ja5V'\n",
        "\n",
        "# Define the fixed definition for correctness\n",
        "CORRECTNESS_DEFINITION = \"Correctness is measured by the semantic similarity between the provided disease description and the suggested disease term. Higher similarity indicates higher correctness.\"\n",
        "\n",
        "# Load your dataset\n",
        "df = pd.read_excel('/content/Train4.xlsx')\n",
        "\n",
        "# Define a function to get embeddings for a list of texts\n",
        "def get_embeddings(texts, engine=\"text-embedding-ada-002\"):\n",
        "    try:\n",
        "        response = openai.Embedding.create(input=texts, engine=engine)\n",
        "        embeddings = [item['embedding'] for item in response['data']]\n",
        "        return embeddings\n",
        "    except Exception as e:\n",
        "        print(f\"Error getting embeddings: {e}\")\n",
        "        return [None] * len(texts)\n",
        "\n",
        "# Define a function to perform a preliminary context check\n",
        "def preliminary_context_check(description, disease):\n",
        "    try:\n",
        "        response = openai.Completion.create(\n",
        "            model=\"gpt-3.5-turbo-instruct\",\n",
        "            prompt=f\"{CORRECTNESS_DEFINITION}\\n\\nIs the term '{disease}' contextually relevant to the description '{description}'? If not, suggest the most appropriate term.\",\n",
        "            max_tokens=50,\n",
        "            temperature=0.3\n",
        "        )\n",
        "        corrected_term = response.choices[0].text.strip()\n",
        "        return corrected_term\n",
        "    except Exception as e:\n",
        "        print(f\"Error querying OpenAI for preliminary context: {e}\")\n",
        "        return \"Error\"\n",
        "\n",
        "# Apply the validation function to the dataset\n",
        "correctness_values = []\n",
        "total_iterations = len(df)  # Get total number of iterations\n",
        "with tqdm(total=total_iterations, desc=\"Processing\") as pbar:  # Initialize progress bar\n",
        "    for _, row in df.iterrows():\n",
        "        corrected_term = preliminary_context_check(row['Description'], row['Disease'])\n",
        "        description_embedding = get_embeddings([row['Description']])\n",
        "        disease_embedding = get_embeddings([row['Disease']])\n",
        "        description_emb = description_embedding[0]\n",
        "        disease_emb = disease_embedding[0]\n",
        "        if description_emb is not None and disease_emb is not None:\n",
        "            similarity = 1 - cosine(description_emb, disease_emb)\n",
        "            correctness_values.append(similarity)\n",
        "        else:\n",
        "            correctness_values.append(None)\n",
        "        pbar.update(1)  # Update progress bar\n",
        "\n",
        "# Calculate Correctness Score\n",
        "correctness_score = sum(correctness_values) / len(correctness_values)\n",
        "\n",
        "# Print Correctness Score and the definition\n",
        "print(f\"Definition for Correctness: {CORRECTNESS_DEFINITION}\")\n",
        "print(f\"Correctness Score: {round(correctness_score, 4)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IsmPBAYjZuUM",
        "outputId": "f5cbb5f9-78e2-405b-ddbd-199966bb2200"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing: 100%|██████████| 4801/4801 [1:06:45<00:00,  1.20it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Definition for Correctness: Correctness is measured by the semantic similarity between the provided disease description and the suggested disease term. Higher similarity indicates higher correctness.\n",
            "Correctness Score: 0.9643\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import openai\n",
        "from scipy.spatial.distance import cosine\n",
        "from tqdm import tqdm  # Import tqdm for progress tracking\n",
        "\n",
        "# Set your OpenAI API key\n",
        "openai.api_key = 'sk-proj-9hKjQtT6e6orqiwwd10jT3BlbkFJkW2KlXLZ4Ir2B7s6ja5V'\n",
        "\n",
        "# Define the fixed definition for correctness\n",
        "CORRECTNESS_DEFINITION = \"Correctness is measured by the semantic similarity between the provided disease description and the suggested disease term. Higher similarity indicates higher correctness.\"\n",
        "\n",
        "# Load your dataset\n",
        "df = pd.read_excel('/content/Train5.xlsx')\n",
        "\n",
        "# Define a function to get embeddings for a list of texts\n",
        "def get_embeddings(texts, engine=\"text-embedding-ada-002\"):\n",
        "    try:\n",
        "        response = openai.Embedding.create(input=texts, engine=engine)\n",
        "        embeddings = [item['embedding'] for item in response['data']]\n",
        "        return embeddings\n",
        "    except Exception as e:\n",
        "        print(f\"Error getting embeddings: {e}\")\n",
        "        return [None] * len(texts)\n",
        "\n",
        "# Define a function to perform a preliminary context check\n",
        "def preliminary_context_check(description, disease):\n",
        "    try:\n",
        "        response = openai.Completion.create(\n",
        "            model=\"gpt-3.5-turbo-instruct\",\n",
        "            prompt=f\"{CORRECTNESS_DEFINITION}\\n\\nIs the term '{disease}' contextually relevant to the description '{description}'? If not, suggest the most appropriate term.\",\n",
        "            max_tokens=50,\n",
        "            temperature=0.3\n",
        "        )\n",
        "        corrected_term = response.choices[0].text.strip()\n",
        "        return corrected_term\n",
        "    except Exception as e:\n",
        "        print(f\"Error querying OpenAI for preliminary context: {e}\")\n",
        "        return \"Error\"\n",
        "\n",
        "# Apply the validation function to the dataset\n",
        "correctness_values = []\n",
        "total_iterations = len(df)  # Get total number of iterations\n",
        "with tqdm(total=total_iterations, desc=\"Processing\") as pbar:  # Initialize progress bar\n",
        "    for _, row in df.iterrows():\n",
        "        corrected_term = preliminary_context_check(row['Description'], row['Disease'])\n",
        "        description_embedding = get_embeddings([row['Description']])\n",
        "        disease_embedding = get_embeddings([row['Disease']])\n",
        "        description_emb = description_embedding[0]\n",
        "        disease_emb = disease_embedding[0]\n",
        "        if description_emb is not None and disease_emb is not None:\n",
        "            similarity = 1 - cosine(description_emb, disease_emb)\n",
        "            correctness_values.append(similarity)\n",
        "        else:\n",
        "            correctness_values.append(None)\n",
        "        pbar.update(1)  # Update progress bar\n",
        "\n",
        "# Calculate Correctness Score\n",
        "correctness_score = sum(correctness_values) / len(correctness_values)\n",
        "\n",
        "# Print Correctness Score and the definition\n",
        "print(f\"Definition for Correctness: {CORRECTNESS_DEFINITION}\")\n",
        "print(f\"Correctness Score: {round(correctness_score, 4)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9GqWCpCZzR4W",
        "outputId": "61542bca-f623-485a-88a9-33d842d3a3f0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing: 100%|██████████| 4819/4819 [1:08:07<00:00,  1.18it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Definition for Correctness: Correctness is measured by the semantic similarity between the provided disease description and the suggested disease term. Higher similarity indicates higher correctness.\n",
            "Correctness Score: 0.964\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import openai\n",
        "from scipy.spatial.distance import cosine\n",
        "from tqdm import tqdm  # Import tqdm for progress tracking\n",
        "\n",
        "# Set your OpenAI API key\n",
        "openai.api_key = 'sk-proj-9hKjQtT6e6orqiwwd10jT3BlbkFJkW2KlXLZ4Ir2B7s6ja5V'\n",
        "\n",
        "# Define the fixed definition for correctness\n",
        "CORRECTNESS_DEFINITION = \"Correctness is measured by the semantic similarity between the provided disease description and the suggested disease term. Higher similarity indicates higher correctness.\"\n",
        "\n",
        "# Load your dataset\n",
        "df = pd.read_excel('/content/Train6.xlsx')\n",
        "\n",
        "# Define a function to get embeddings for a list of texts\n",
        "def get_embeddings(texts, engine=\"text-embedding-ada-002\"):\n",
        "    try:\n",
        "        response = openai.Embedding.create(input=texts, engine=engine)\n",
        "        embeddings = [item['embedding'] for item in response['data']]\n",
        "        return embeddings\n",
        "    except Exception as e:\n",
        "        print(f\"Error getting embeddings: {e}\")\n",
        "        return [None] * len(texts)\n",
        "\n",
        "# Define a function to perform a preliminary context check\n",
        "def preliminary_context_check(description, disease):\n",
        "    try:\n",
        "        response = openai.Completion.create(\n",
        "            model=\"gpt-3.5-turbo-instruct\",\n",
        "            prompt=f\"{CORRECTNESS_DEFINITION}\\n\\nIs the term '{disease}' contextually relevant to the description '{description}'? If not, suggest the most appropriate term.\",\n",
        "            max_tokens=50,\n",
        "            temperature=0.3\n",
        "        )\n",
        "        corrected_term = response.choices[0].text.strip()\n",
        "        return corrected_term\n",
        "    except Exception as e:\n",
        "        print(f\"Error querying OpenAI for preliminary context: {e}\")\n",
        "        return \"Error\"\n",
        "\n",
        "# Apply the validation function to the dataset\n",
        "correctness_values = []\n",
        "total_iterations = len(df)  # Get total number of iterations\n",
        "with tqdm(total=total_iterations, desc=\"Processing\") as pbar:  # Initialize progress bar\n",
        "    for _, row in df.iterrows():\n",
        "        corrected_term = preliminary_context_check(row['Description'], row['Disease'])\n",
        "        description_embedding = get_embeddings([row['Description']])\n",
        "        disease_embedding = get_embeddings([row['Disease']])\n",
        "        description_emb = description_embedding[0]\n",
        "        disease_emb = disease_embedding[0]\n",
        "        if description_emb is not None and disease_emb is not None:\n",
        "            similarity = 1 - cosine(description_emb, disease_emb)\n",
        "            correctness_values.append(similarity)\n",
        "        else:\n",
        "            correctness_values.append(None)\n",
        "        pbar.update(1)  # Update progress bar\n",
        "\n",
        "# Calculate Correctness Score\n",
        "correctness_score = sum(correctness_values) / len(correctness_values)\n",
        "\n",
        "# Print Correctness Score and the definition\n",
        "print(f\"Definition for Correctness: {CORRECTNESS_DEFINITION}\")\n",
        "print(f\"Correctness Score: {round(correctness_score, 4)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VaRoNNo4FzYu",
        "outputId": "04cdac43-0dd8-46f4-dde1-27c3de33bee1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing: 100%|██████████| 4790/4790 [1:06:58<00:00,  1.19it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Definition for Correctness: Correctness is measured by the semantic similarity between the provided disease description and the suggested disease term. Higher similarity indicates higher correctness.\n",
            "Correctness Score: 0.9648\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import openai\n",
        "from scipy.spatial.distance import cosine\n",
        "from tqdm import tqdm  # Import tqdm for progress tracking\n",
        "\n",
        "# Set your OpenAI API key\n",
        "openai.api_key = 'sk-proj-9hKjQtT6e6orqiwwd10jT3BlbkFJkW2KlXLZ4Ir2B7s6ja5V'\n",
        "\n",
        "# Define the fixed definition for correctness\n",
        "CORRECTNESS_DEFINITION = \"Correctness is measured by the semantic similarity between the provided disease description and the suggested disease term. Higher similarity indicates higher correctness.\"\n",
        "\n",
        "# Load your dataset\n",
        "df = pd.read_excel('/content/Train7.xlsx')\n",
        "\n",
        "# Define a function to get embeddings for a list of texts\n",
        "def get_embeddings(texts, engine=\"text-embedding-ada-002\"):\n",
        "    try:\n",
        "        response = openai.Embedding.create(input=texts, engine=engine)\n",
        "        embeddings = [item['embedding'] for item in response['data']]\n",
        "        return embeddings\n",
        "    except Exception as e:\n",
        "        print(f\"Error getting embeddings: {e}\")\n",
        "        return [None] * len(texts)\n",
        "\n",
        "# Define a function to perform a preliminary context check\n",
        "def preliminary_context_check(description, disease):\n",
        "    try:\n",
        "        response = openai.Completion.create(\n",
        "            model=\"gpt-3.5-turbo-instruct\",\n",
        "            prompt=f\"{CORRECTNESS_DEFINITION}\\n\\nIs the term '{disease}' contextually relevant to the description '{description}'? If not, suggest the most appropriate term.\",\n",
        "            max_tokens=50,\n",
        "            temperature=0.3\n",
        "        )\n",
        "        corrected_term = response.choices[0].text.strip()\n",
        "        return corrected_term\n",
        "    except Exception as e:\n",
        "        print(f\"Error querying OpenAI for preliminary context: {e}\")\n",
        "        return \"Error\"\n",
        "\n",
        "# Apply the validation function to the dataset\n",
        "correctness_values = []\n",
        "total_iterations = len(df)  # Get total number of iterations\n",
        "with tqdm(total=total_iterations, desc=\"Processing\") as pbar:  # Initialize progress bar\n",
        "    for _, row in df.iterrows():\n",
        "        corrected_term = preliminary_context_check(row['Description'], row['Disease'])\n",
        "        description_embedding = get_embeddings([row['Description']])\n",
        "        disease_embedding = get_embeddings([row['Disease']])\n",
        "        description_emb = description_embedding[0]\n",
        "        disease_emb = disease_embedding[0]\n",
        "        if description_emb is not None and disease_emb is not None:\n",
        "            similarity = 1 - cosine(description_emb, disease_emb)\n",
        "            correctness_values.append(similarity)\n",
        "        else:\n",
        "            correctness_values.append(None)\n",
        "        pbar.update(1)  # Update progress bar\n",
        "\n",
        "# Calculate Correctness Score\n",
        "correctness_score = sum(correctness_values) / len(correctness_values)\n",
        "\n",
        "# Print Correctness Score and the definition\n",
        "print(f\"Definition for Correctness: {CORRECTNESS_DEFINITION}\")\n",
        "print(f\"Correctness Score: {round(correctness_score, 4)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sn63ltUsXkcL",
        "outputId": "39e2ed05-97dc-4be9-b65e-6d21a1cf8d3b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing: 100%|██████████| 4788/4788 [1:07:37<00:00,  1.18it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Definition for Correctness: Correctness is measured by the semantic similarity between the provided disease description and the suggested disease term. Higher similarity indicates higher correctness.\n",
            "Correctness Score: 0.9647\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import openai\n",
        "from scipy.spatial.distance import cosine\n",
        "from tqdm import tqdm  # Import tqdm for progress tracking\n",
        "\n",
        "# Set your OpenAI API key\n",
        "openai.api_key = 'sk-proj-9hKjQtT6e6orqiwwd10jT3BlbkFJkW2KlXLZ4Ir2B7s6ja5V'\n",
        "\n",
        "# Define the fixed definition for correctness\n",
        "CORRECTNESS_DEFINITION = \"Correctness is measured by the semantic similarity between the provided disease description and the suggested disease term. Higher similarity indicates higher correctness.\"\n",
        "\n",
        "# Load your dataset\n",
        "df = pd.read_excel('/content/Train8.xlsx')\n",
        "\n",
        "# Define a function to get embeddings for a list of texts\n",
        "def get_embeddings(texts, engine=\"text-embedding-ada-002\"):\n",
        "    try:\n",
        "        response = openai.Embedding.create(input=texts, engine=engine)\n",
        "        embeddings = [item['embedding'] for item in response['data']]\n",
        "        return embeddings\n",
        "    except Exception as e:\n",
        "        print(f\"Error getting embeddings: {e}\")\n",
        "        return [None] * len(texts)\n",
        "\n",
        "# Define a function to perform a preliminary context check\n",
        "def preliminary_context_check(description, disease):\n",
        "    try:\n",
        "        response = openai.Completion.create(\n",
        "            model=\"gpt-3.5-turbo-instruct\",\n",
        "            prompt=f\"{CORRECTNESS_DEFINITION}\\n\\nIs the term '{disease}' contextually relevant to the description '{description}'? If not, suggest the most appropriate term.\",\n",
        "            max_tokens=50,\n",
        "            temperature=0.3\n",
        "        )\n",
        "        corrected_term = response.choices[0].text.strip()\n",
        "        return corrected_term\n",
        "    except Exception as e:\n",
        "        print(f\"Error querying OpenAI for preliminary context: {e}\")\n",
        "        return \"Error\"\n",
        "\n",
        "# Apply the validation function to the dataset\n",
        "correctness_values = []\n",
        "total_iterations = len(df)  # Get total number of iterations\n",
        "with tqdm(total=total_iterations, desc=\"Processing\") as pbar:  # Initialize progress bar\n",
        "    for _, row in df.iterrows():\n",
        "        corrected_term = preliminary_context_check(row['Description'], row['Disease'])\n",
        "        description_embedding = get_embeddings([row['Description']])\n",
        "        disease_embedding = get_embeddings([row['Disease']])\n",
        "        description_emb = description_embedding[0]\n",
        "        disease_emb = disease_embedding[0]\n",
        "        if description_emb is not None and disease_emb is not None:\n",
        "            similarity = 1 - cosine(description_emb, disease_emb)\n",
        "            correctness_values.append(similarity)\n",
        "        else:\n",
        "            correctness_values.append(None)\n",
        "        pbar.update(1)  # Update progress bar\n",
        "\n",
        "# Calculate Correctness Score\n",
        "correctness_score = sum(correctness_values) / len(correctness_values)\n",
        "\n",
        "# Print Correctness Score and the definition\n",
        "print(f\"Definition for Correctness: {CORRECTNESS_DEFINITION}\")\n",
        "print(f\"Correctness Score: {round(correctness_score, 4)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kN_XDD7qYHxf",
        "outputId": "dda1a6f5-3457-4dc1-ecab-e207c62588df"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing: 100%|██████████| 4812/4812 [1:09:54<00:00,  1.15it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Definition for Correctness: Correctness is measured by the semantic similarity between the provided disease description and the suggested disease term. Higher similarity indicates higher correctness.\n",
            "Correctness Score: 0.964\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import openai\n",
        "from scipy.spatial.distance import cosine\n",
        "from tqdm import tqdm  # Import tqdm for progress tracking\n",
        "\n",
        "# Set your OpenAI API key\n",
        "openai.api_key = 'sk-proj-9hKjQtT6e6orqiwwd10jT3BlbkFJkW2KlXLZ4Ir2B7s6ja5V'\n",
        "\n",
        "# Define the fixed definition for correctness\n",
        "CORRECTNESS_DEFINITION = \"Correctness is measured by the semantic similarity between the provided disease description and the suggested disease term. Higher similarity indicates higher correctness.\"\n",
        "\n",
        "# Load your dataset\n",
        "df = pd.read_excel('/content/Train9.xlsx')\n",
        "\n",
        "# Define a function to get embeddings for a list of texts\n",
        "def get_embeddings(texts, engine=\"text-embedding-ada-002\"):\n",
        "    try:\n",
        "        response = openai.Embedding.create(input=texts, engine=engine)\n",
        "        embeddings = [item['embedding'] for item in response['data']]\n",
        "        return embeddings\n",
        "    except Exception as e:\n",
        "        print(f\"Error getting embeddings: {e}\")\n",
        "        return [None] * len(texts)\n",
        "\n",
        "# Define a function to perform a preliminary context check\n",
        "def preliminary_context_check(description, disease):\n",
        "    try:\n",
        "        response = openai.Completion.create(\n",
        "            model=\"gpt-3.5-turbo-instruct\",\n",
        "            prompt=f\"{CORRECTNESS_DEFINITION}\\n\\nIs the term '{disease}' contextually relevant to the description '{description}'? If not, suggest the most appropriate term.\",\n",
        "            max_tokens=50,\n",
        "            temperature=0.3\n",
        "        )\n",
        "        corrected_term = response.choices[0].text.strip()\n",
        "        return corrected_term\n",
        "    except Exception as e:\n",
        "        print(f\"Error querying OpenAI for preliminary context: {e}\")\n",
        "        return \"Error\"\n",
        "\n",
        "# Apply the validation function to the dataset\n",
        "correctness_values = []\n",
        "total_iterations = len(df)  # Get total number of iterations\n",
        "with tqdm(total=total_iterations, desc=\"Processing\") as pbar:  # Initialize progress bar\n",
        "    for _, row in df.iterrows():\n",
        "        corrected_term = preliminary_context_check(row['Description'], row['Disease'])\n",
        "        description_embedding = get_embeddings([row['Description']])\n",
        "        disease_embedding = get_embeddings([row['Disease']])\n",
        "        description_emb = description_embedding[0]\n",
        "        disease_emb = disease_embedding[0]\n",
        "        if description_emb is not None and disease_emb is not None:\n",
        "            similarity = 1 - cosine(description_emb, disease_emb)\n",
        "            correctness_values.append(similarity)\n",
        "        else:\n",
        "            correctness_values.append(None)\n",
        "        pbar.update(1)  # Update progress bar\n",
        "\n",
        "# Calculate Correctness Score\n",
        "correctness_score = sum(correctness_values) / len(correctness_values)\n",
        "\n",
        "# Print Correctness Score and the definition\n",
        "print(f\"Definition for Correctness: {CORRECTNESS_DEFINITION}\")\n",
        "print(f\"Correctness Score: {round(correctness_score, 4)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OeWFWiQhE1w6",
        "outputId": "c36a06ba-2ea0-472c-a1ce-d55fca120db7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Correctness Score: 0.96297\n"
          ]
        }
      ],
      "source": [
        "# Output values from code cells\n",
        "output_scores = [0.96, 0.96, 0.96, 0.9641, 0.9638, 0.9643, 0.964, 0.9648, 0.9647, 0.964]  # Replace these values with your actual output correctness scores\n",
        "\n",
        "# Calculate the average correctness score\n",
        "avg_correctness_score = sum(output_scores) / len(output_scores)\n",
        "\n",
        "# Print the average correctness score\n",
        "print(f\"Average Correctness Score: {round(avg_correctness_score, 5)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLWhyuxhh06H",
        "outputId": "ae763402-9671-4f0c-b0b9-95f59187446b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 100.00%  Entries Processed: 4816/4816\n",
            "Uniqueness Score for Train0.xlsx: 0.6341\n",
            "Progress: 100.00%  Entries Processed: 4817/4817\n",
            "Uniqueness Score for Train1.xlsx: 0.6367\n",
            "Progress: 100.00%  Entries Processed: 4791/4791\n",
            "Uniqueness Score for Train2.xlsx: 0.6345\n",
            "Progress: 100.00%  Entries Processed: 4812/4812\n",
            "Uniqueness Score for Train3.xlsx: 0.6330\n",
            "Progress: 100.00%  Entries Processed: 4811/4811\n",
            "Uniqueness Score for Train4.xlsx: 0.6344\n",
            "Progress: 100.00%  Entries Processed: 4801/4801\n",
            "Uniqueness Score for Train5.xlsx: 0.6336\n",
            "Progress: 100.00%  Entries Processed: 4819/4819\n",
            "Uniqueness Score for Train6.xlsx: 0.6339\n",
            "Progress: 100.00%  Entries Processed: 4790/4790\n",
            "Uniqueness Score for Train7.xlsx: 0.6351\n",
            "Progress: 100.00%  Entries Processed: 4788/4788\n",
            "Uniqueness Score for Train8.xlsx: 0.6328\n",
            "Progress: 100.00%  Entries Processed: 4812/4812\n",
            "Uniqueness Score for Train9.xlsx: 0.6363\n",
            "\n",
            "Average Uniqueness Score: 0.6345\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import openai\n",
        "import os\n",
        "from tqdm import tqdm  # Import tqdm for progress tracking\n",
        "\n",
        "# Set your OpenAI API key\n",
        "openai.api_key = 'sk-proj-9hKjQtT6e6orqiwwd10jT3BlbkFJkW2KlXLZ4Ir2B7s6ja5V'\n",
        "\n",
        "# Define the uniqueness definition\n",
        "UNIQUENESS_DEFINITION = \"Uniqueness is determined by assessing whether each disease-description combination in the dataset is distinct from all other combinations. A unique pair consists of a disease and its corresponding description that does not have an exact match with any other pair in the dataset.\"\n",
        "\n",
        "# Define a function to determine if an entry is unique or a duplicate using ChatGPT\n",
        "def is_unique(entry):\n",
        "    try:\n",
        "        # Generate a prompt asking ChatGPT to check for duplicates of the disease-description combination\n",
        "        prompt = f\"{UNIQUENESS_DEFINITION}\\n\\nAre there any duplicates of the combination 'disease': '{entry['Disease']}' and 'description': '{entry['Description']}' in the dataset?\"\n",
        "\n",
        "        # Call OpenAI API to generate a response\n",
        "        response = openai.Completion.create(\n",
        "            engine=\"gpt-3.5-turbo-instruct\",  # Specify the GPT model\n",
        "            prompt=prompt,\n",
        "            max_tokens=50  # Control the length of the response\n",
        "        )\n",
        "\n",
        "        # Interpret the response\n",
        "        if \"Yes\" in response.choices[0].text.strip():\n",
        "            return False\n",
        "        elif \"No\" in response.choices[0].text.strip():\n",
        "            return True\n",
        "        else:\n",
        "            return False  # Return False if response is unclear\n",
        "    except Exception as e:\n",
        "        print(f\"Error determining uniqueness: {e}\")\n",
        "        return False  # Return False in case of an error\n",
        "\n",
        "# Define a function to extract unique pairs of disease and description\n",
        "def get_unique_pairs(df):\n",
        "    unique_pairs = set()  # Store unique pairs in a set to avoid duplicates\n",
        "    for i, (_, row) in enumerate(df.iterrows(), start=1):\n",
        "        pair = (row['Disease'], row['Description'])\n",
        "        unique_pairs.add(pair)\n",
        "        # Calculate progress\n",
        "        progress = i / len(df) * 100\n",
        "        print(f\"Progress: {progress:.2f}%  Entries Processed: {i}/{len(df)}\", end='\\r')\n",
        "    return unique_pairs\n",
        "\n",
        "# Define a function to calculate uniqueness score for a dataset\n",
        "def calculate_uniqueness_score(dataset_path):\n",
        "    file_name = os.path.basename(dataset_path)\n",
        "    df = pd.read_excel(dataset_path)\n",
        "    unique_pairs = get_unique_pairs(df)\n",
        "    total_entries = len(df)\n",
        "    unique_pairs_count = len(unique_pairs)\n",
        "    uniqueness_score = unique_pairs_count / total_entries\n",
        "    return uniqueness_score, file_name\n",
        "\n",
        "# List of dataset paths\n",
        "dataset_paths = ['/content/Train0.xlsx', '/content/Train1.xlsx', '/content/Train2.xlsx', '/content/Train3.xlsx', '/content/Train4.xlsx', '/content/Train5.xlsx', '/content/Train6.xlsx', '/content/Train7.xlsx', '/content/Train8.xlsx', '/content/Train9.xlsx']\n",
        "\n",
        "# Calculate uniqueness score for each dataset and print individual scores\n",
        "total_uniqueness_score = 0\n",
        "for dataset_path in dataset_paths:\n",
        "    file_name = os.path.basename(dataset_path)\n",
        "    print(f\"Processing {file_name}:\", end='')\n",
        "    uniqueness_score, _ = calculate_uniqueness_score(dataset_path)\n",
        "    total_uniqueness_score += uniqueness_score\n",
        "    print(f\"\\nUniqueness Score for {file_name}: {uniqueness_score:.4f}\")\n",
        "\n",
        "# Calculate and print average uniqueness score\n",
        "avg_uniqueness_score = total_uniqueness_score / len(dataset_paths)\n",
        "print(f\"\\nAverage Uniqueness Score: {avg_uniqueness_score:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXbOsJdTmnUR",
        "outputId": "4b7b8694-cde9-4354-9d97-ff3716fc7f32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comprehensiveness Score: 0.8063185595778304\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import openai\n",
        "\n",
        "# Set your OpenAI API key\n",
        "openai.api_key = 'sk-proj-9hKjQtT6e6orqiwwd10jT3BlbkFJkW2KlXLZ4Ir2B7s6ja5V'\n",
        "\n",
        "# Load the dataset\n",
        "dataset_path = '/content/Train0.xlsx'  # Update with the path to your dataset\n",
        "df = pd.read_excel(dataset_path)\n",
        "\n",
        "# Define a function to assess the comprehensiveness of a description using ChatGPT\n",
        "def assess_comprehensiveness(disease, description):\n",
        "    try:\n",
        "        # Generate a prompt asking ChatGPT to assess the quality and completeness of the description\n",
        "        prompt = f\"Assess the comprehensiveness of the following description for the disease '{disease}':\\n'{description}'\"\n",
        "\n",
        "        # Use ChatGPT to generate a response\n",
        "        response = openai.Completion.create(\n",
        "            engine=\"gpt-3.5-turbo-instruct\",\n",
        "            prompt=prompt,\n",
        "            max_tokens=100,\n",
        "            temperature=0.3\n",
        "        )\n",
        "\n",
        "        # Extract the completion text from the response\n",
        "        completion_text = response.choices[0].text.strip()\n",
        "\n",
        "        # For simplicity, let's assume the comprehensiveness score is the length of the completion\n",
        "        # You may need to adjust this based on the expected response format\n",
        "        comprehensiveness_score = len(completion_text)\n",
        "\n",
        "        return comprehensiveness_score\n",
        "    except Exception as e:\n",
        "        print(f\"Error assessing comprehensiveness: {e}\")\n",
        "        return None\n",
        "\n",
        "# Iterate over each row in the dataset and assess comprehensiveness\n",
        "comprehensiveness_scores = []\n",
        "for index, row in df.iterrows():\n",
        "    score = assess_comprehensiveness(row['Disease'], row['Description'])\n",
        "    if score is not None:\n",
        "        comprehensiveness_scores.append(score)\n",
        "\n",
        "# Normalize the scores to a scale of 0 to 1\n",
        "max_score = max(comprehensiveness_scores)\n",
        "normalized_scores = [score / max_score for score in comprehensiveness_scores]\n",
        "\n",
        "# Calculate the average comprehensiveness score for the dataset\n",
        "if normalized_scores:\n",
        "    average_comprehensiveness_score = sum(normalized_scores) / len(normalized_scores)\n",
        "    print(\"Comprehensiveness Score:\", average_comprehensiveness_score)\n",
        "else:\n",
        "    print(\"No comprehensiveness scores calculated.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jxVgQfmQHvq",
        "outputId": "5f9a81b8-b714-4010-8bc8-016e9e833dea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comprehensiveness Score: 0.7994336289186412\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import openai\n",
        "\n",
        "# Set your OpenAI API key\n",
        "openai.api_key = 'sk-proj-9hKjQtT6e6orqiwwd10jT3BlbkFJkW2KlXLZ4Ir2B7s6ja5V'\n",
        "\n",
        "# Load the dataset\n",
        "dataset_path = '/content/Train9.xlsx'  # Update with the path to your dataset\n",
        "df = pd.read_excel(dataset_path)\n",
        "\n",
        "# Define a function to assess the comprehensiveness of a description using ChatGPT\n",
        "def assess_comprehensiveness(disease, description):\n",
        "    try:\n",
        "        # Generate a prompt asking ChatGPT to assess the quality and completeness of the description\n",
        "        prompt = f\"Assess the comprehensiveness of the following description for the disease '{disease}':\\n'{description}'\"\n",
        "\n",
        "        # Use ChatGPT to generate a response\n",
        "        response = openai.Completion.create(\n",
        "            engine=\"gpt-3.5-turbo-instruct\",\n",
        "            prompt=prompt,\n",
        "            max_tokens=100,\n",
        "            temperature=0.3\n",
        "        )\n",
        "\n",
        "        # Extract the completion text from the response\n",
        "        completion_text = response.choices[0].text.strip()\n",
        "\n",
        "        # For simplicity, let's assume the comprehensiveness score is the length of the completion\n",
        "        # You may need to adjust this based on the expected response format\n",
        "        comprehensiveness_score = len(completion_text)\n",
        "\n",
        "        return comprehensiveness_score\n",
        "    except Exception as e:\n",
        "        print(f\"Error assessing comprehensiveness: {e}\")\n",
        "        return None\n",
        "\n",
        "# Iterate over each row in the dataset and assess comprehensiveness\n",
        "comprehensiveness_scores = []\n",
        "for index, row in df.iterrows():\n",
        "    score = assess_comprehensiveness(row['Disease'], row['Description'])\n",
        "    if score is not None:\n",
        "        comprehensiveness_scores.append(score)\n",
        "\n",
        "# Normalize the scores to a scale of 0 to 1\n",
        "max_score = max(comprehensiveness_scores)\n",
        "normalized_scores = [score / max_score for score in comprehensiveness_scores]\n",
        "\n",
        "# Calculate the average comprehensiveness score for the dataset\n",
        "if normalized_scores:\n",
        "    average_comprehensiveness_score = sum(normalized_scores) / len(normalized_scores)\n",
        "    print(\"Comprehensiveness Score:\", average_comprehensiveness_score)\n",
        "else:\n",
        "    print(\"No comprehensiveness scores calculated.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BbHJoFBejpZB",
        "outputId": "a1e3c8a6-fe43-4989-8444-3420ca73891c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comprehensiveness Score: 0.8197914303974229\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import openai\n",
        "\n",
        "# Set your OpenAI API key\n",
        "openai.api_key = 'sk-proj-9hKjQtT6e6orqiwwd10jT3BlbkFJkW2KlXLZ4Ir2B7s6ja5V'\n",
        "\n",
        "# Load the dataset\n",
        "dataset_path = '/content/Train2.xlsx'  # Update with the path to your dataset\n",
        "df = pd.read_excel(dataset_path)\n",
        "\n",
        "# Define a function to assess the comprehensiveness of a description using ChatGPT\n",
        "def assess_comprehensiveness(disease, description):\n",
        "    try:\n",
        "        # Generate a prompt asking ChatGPT to assess the quality and completeness of the description\n",
        "        prompt = f\"Assess the comprehensiveness of the following description for the disease '{disease}':\\n'{description}'\"\n",
        "\n",
        "        # Use ChatGPT to generate a response\n",
        "        response = openai.Completion.create(\n",
        "            engine=\"gpt-3.5-turbo-instruct\",\n",
        "            prompt=prompt,\n",
        "            max_tokens=100,\n",
        "            temperature=0.3\n",
        "        )\n",
        "\n",
        "        # Extract the completion text from the response\n",
        "        completion_text = response.choices[0].text.strip()\n",
        "\n",
        "        # For simplicity, let's assume the comprehensiveness score is the length of the completion\n",
        "        # You may need to adjust this based on the expected response format\n",
        "        comprehensiveness_score = len(completion_text)\n",
        "\n",
        "        return comprehensiveness_score\n",
        "    except Exception as e:\n",
        "        print(f\"Error assessing comprehensiveness: {e}\")\n",
        "        return None\n",
        "\n",
        "# Iterate over each row in the dataset and assess comprehensiveness\n",
        "comprehensiveness_scores = []\n",
        "for index, row in df.iterrows():\n",
        "    score = assess_comprehensiveness(row['Disease'], row['Description'])\n",
        "    if score is not None:\n",
        "        comprehensiveness_scores.append(score)\n",
        "\n",
        "# Normalize the scores to a scale of 0 to 1\n",
        "max_score = max(comprehensiveness_scores)\n",
        "normalized_scores = [score / max_score for score in comprehensiveness_scores]\n",
        "\n",
        "# Calculate the average comprehensiveness score for the dataset\n",
        "if normalized_scores:\n",
        "    average_comprehensiveness_score = sum(normalized_scores) / len(normalized_scores)\n",
        "    print(\"Comprehensiveness Score:\", average_comprehensiveness_score)\n",
        "else:\n",
        "    print(\"No comprehensiveness scores calculated.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLkUXTzLlrBt",
        "outputId": "131398e7-3319-4184-c882-d62980db17fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comprehensiveness Score: 0.8213285312193753\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import openai\n",
        "\n",
        "# Set your OpenAI API key\n",
        "openai.api_key = 'sk-proj-9hKjQtT6e6orqiwwd10jT3BlbkFJkW2KlXLZ4Ir2B7s6ja5V'\n",
        "\n",
        "# Load the dataset\n",
        "dataset_path = '/content/Train1.xlsx'  # Update with the path to your dataset\n",
        "df = pd.read_excel(dataset_path)\n",
        "\n",
        "# Define a function to assess the comprehensiveness of a description using ChatGPT\n",
        "def assess_comprehensiveness(disease, description):\n",
        "    try:\n",
        "        # Generate a prompt asking ChatGPT to assess the quality and completeness of the description\n",
        "        prompt = f\"Assess the comprehensiveness of the following description for the disease '{disease}':\\n'{description}'\"\n",
        "\n",
        "        # Use ChatGPT to generate a response\n",
        "        response = openai.Completion.create(\n",
        "            engine=\"gpt-3.5-turbo-instruct\",\n",
        "            prompt=prompt,\n",
        "            max_tokens=100,\n",
        "            temperature=0.3\n",
        "        )\n",
        "\n",
        "        # Extract the completion text from the response\n",
        "        completion_text = response.choices[0].text.strip()\n",
        "\n",
        "        # For simplicity, let's assume the comprehensiveness score is the length of the completion\n",
        "        # You may need to adjust this based on the expected response format\n",
        "        comprehensiveness_score = len(completion_text)\n",
        "\n",
        "        return comprehensiveness_score\n",
        "    except Exception as e:\n",
        "        print(f\"Error assessing comprehensiveness: {e}\")\n",
        "        return None\n",
        "\n",
        "# Iterate over each row in the dataset and assess comprehensiveness\n",
        "comprehensiveness_scores = []\n",
        "for index, row in df.iterrows():\n",
        "    score = assess_comprehensiveness(row['Disease'], row['Description'])\n",
        "    if score is not None:\n",
        "        comprehensiveness_scores.append(score)\n",
        "\n",
        "# Normalize the scores to a scale of 0 to 1\n",
        "max_score = max(comprehensiveness_scores)\n",
        "normalized_scores = [score / max_score for score in comprehensiveness_scores]\n",
        "\n",
        "# Calculate the average comprehensiveness score for the dataset\n",
        "if normalized_scores:\n",
        "    average_comprehensiveness_score = sum(normalized_scores) / len(normalized_scores)\n",
        "    print(\"Comprehensiveness Score:\", average_comprehensiveness_score)\n",
        "else:\n",
        "    print(\"No comprehensiveness scores calculated.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIhdvaSBAUWx",
        "outputId": "d46a52e5-ad28-4164-9d40-b84e505798ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comprehensiveness Score: 0.8235835644533533\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import openai\n",
        "\n",
        "# Set your OpenAI API key\n",
        "openai.api_key = 'sk-proj-9hKjQtT6e6orqiwwd10jT3BlbkFJkW2KlXLZ4Ir2B7s6ja5V'\n",
        "\n",
        "# Load the dataset\n",
        "dataset_path = '/content/Train3.xlsx'  # Update with the path to your dataset\n",
        "df = pd.read_excel(dataset_path)\n",
        "\n",
        "# Define a function to assess the comprehensiveness of a description using ChatGPT\n",
        "def assess_comprehensiveness(disease, description):\n",
        "    try:\n",
        "        # Generate a prompt asking ChatGPT to assess the quality and completeness of the description\n",
        "        prompt = f\"Assess the comprehensiveness of the following description for the disease '{disease}':\\n'{description}'\"\n",
        "\n",
        "        # Use ChatGPT to generate a response\n",
        "        response = openai.Completion.create(\n",
        "            engine=\"gpt-3.5-turbo-instruct\",\n",
        "            prompt=prompt,\n",
        "            max_tokens=100,\n",
        "            temperature=0.3\n",
        "        )\n",
        "\n",
        "        # Extract the completion text from the response\n",
        "        completion_text = response.choices[0].text.strip()\n",
        "\n",
        "        # For simplicity, let's assume the comprehensiveness score is the length of the completion\n",
        "        # You may need to adjust this based on the expected response format\n",
        "        comprehensiveness_score = len(completion_text)\n",
        "\n",
        "        return comprehensiveness_score\n",
        "    except Exception as e:\n",
        "        print(f\"Error assessing comprehensiveness: {e}\")\n",
        "        return None\n",
        "\n",
        "# Iterate over each row in the dataset and assess comprehensiveness\n",
        "comprehensiveness_scores = []\n",
        "for index, row in df.iterrows():\n",
        "    score = assess_comprehensiveness(row['Disease'], row['Description'])\n",
        "    if score is not None:\n",
        "        comprehensiveness_scores.append(score)\n",
        "\n",
        "# Normalize the scores to a scale of 0 to 1\n",
        "max_score = max(comprehensiveness_scores)\n",
        "normalized_scores = [score / max_score for score in comprehensiveness_scores]\n",
        "\n",
        "# Calculate the average comprehensiveness score for the dataset\n",
        "if normalized_scores:\n",
        "    average_comprehensiveness_score = sum(normalized_scores) / len(normalized_scores)\n",
        "    print(\"Comprehensiveness Score:\", average_comprehensiveness_score)\n",
        "else:\n",
        "    print(\"No comprehensiveness scores calculated.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_OkqCK_JcHyY",
        "outputId": "64f01055-c22d-4671-fba6-92f07a258786"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comprehensiveness Score: 0.8226572528675683\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import openai\n",
        "\n",
        "# Set your OpenAI API key\n",
        "openai.api_key = 'sk-proj-9hKjQtT6e6orqiwwd10jT3BlbkFJkW2KlXLZ4Ir2B7s6ja5V'\n",
        "\n",
        "# Load the dataset\n",
        "dataset_path = '/content/Train4.xlsx'  # Update with the path to your dataset\n",
        "df = pd.read_excel(dataset_path)\n",
        "\n",
        "# Define a function to assess the comprehensiveness of a description using ChatGPT\n",
        "def assess_comprehensiveness(disease, description):\n",
        "    try:\n",
        "        # Generate a prompt asking ChatGPT to assess the quality and completeness of the description\n",
        "        prompt = f\"Assess the comprehensiveness of the following description for the disease '{disease}':\\n'{description}'\"\n",
        "\n",
        "        # Use ChatGPT to generate a response\n",
        "        response = openai.Completion.create(\n",
        "            engine=\"gpt-3.5-turbo-instruct\",\n",
        "            prompt=prompt,\n",
        "            max_tokens=100,\n",
        "            temperature=0.3\n",
        "        )\n",
        "\n",
        "        # Extract the completion text from the response\n",
        "        completion_text = response.choices[0].text.strip()\n",
        "\n",
        "        # For simplicity, let's assume the comprehensiveness score is the length of the completion\n",
        "        # You may need to adjust this based on the expected response format\n",
        "        comprehensiveness_score = len(completion_text)\n",
        "\n",
        "        return comprehensiveness_score\n",
        "    except Exception as e:\n",
        "        print(f\"Error assessing comprehensiveness: {e}\")\n",
        "        return None\n",
        "\n",
        "# Iterate over each row in the dataset and assess comprehensiveness\n",
        "comprehensiveness_scores = []\n",
        "for index, row in df.iterrows():\n",
        "    score = assess_comprehensiveness(row['Disease'], row['Description'])\n",
        "    if score is not None:\n",
        "        comprehensiveness_scores.append(score)\n",
        "\n",
        "# Normalize the scores to a scale of 0 to 1\n",
        "max_score = max(comprehensiveness_scores)\n",
        "normalized_scores = [score / max_score for score in comprehensiveness_scores]\n",
        "\n",
        "# Calculate the average comprehensiveness score for the dataset\n",
        "if normalized_scores:\n",
        "    average_comprehensiveness_score = sum(normalized_scores) / len(normalized_scores)\n",
        "    print(\"Comprehensiveness Score:\", average_comprehensiveness_score)\n",
        "else:\n",
        "    print(\"No comprehensiveness scores calculated.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIMTpaC3cJPY",
        "outputId": "8507b312-833d-4216-f570-617dd4a185da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comprehensiveness Score: 0.831655878819794\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import openai\n",
        "\n",
        "# Set your OpenAI API key\n",
        "openai.api_key = 'sk-proj-9hKjQtT6e6orqiwwd10jT3BlbkFJkW2KlXLZ4Ir2B7s6ja5V'\n",
        "\n",
        "# Load the dataset\n",
        "dataset_path = '/content/Train5.xlsx'  # Update with the path to your dataset\n",
        "df = pd.read_excel(dataset_path)\n",
        "\n",
        "# Define a function to assess the comprehensiveness of a description using ChatGPT\n",
        "def assess_comprehensiveness(disease, description):\n",
        "    try:\n",
        "        # Generate a prompt asking ChatGPT to assess the quality and completeness of the description\n",
        "        prompt = f\"Assess the comprehensiveness of the following description for the disease '{disease}':\\n'{description}'\"\n",
        "\n",
        "        # Use ChatGPT to generate a response\n",
        "        response = openai.Completion.create(\n",
        "            engine=\"gpt-3.5-turbo-instruct\",\n",
        "            prompt=prompt,\n",
        "            max_tokens=100,\n",
        "            temperature=0.3\n",
        "        )\n",
        "\n",
        "        # Extract the completion text from the response\n",
        "        completion_text = response.choices[0].text.strip()\n",
        "\n",
        "        # For simplicity, let's assume the comprehensiveness score is the length of the completion\n",
        "        # You may need to adjust this based on the expected response format\n",
        "        comprehensiveness_score = len(completion_text)\n",
        "\n",
        "        return comprehensiveness_score\n",
        "    except Exception as e:\n",
        "        print(f\"Error assessing comprehensiveness: {e}\")\n",
        "        return None\n",
        "\n",
        "# Iterate over each row in the dataset and assess comprehensiveness\n",
        "comprehensiveness_scores = []\n",
        "for index, row in df.iterrows():\n",
        "    score = assess_comprehensiveness(row['Disease'], row['Description'])\n",
        "    if score is not None:\n",
        "        comprehensiveness_scores.append(score)\n",
        "\n",
        "# Normalize the scores to a scale of 0 to 1\n",
        "max_score = max(comprehensiveness_scores)\n",
        "normalized_scores = [score / max_score for score in comprehensiveness_scores]\n",
        "\n",
        "# Calculate the average comprehensiveness score for the dataset\n",
        "if normalized_scores:\n",
        "    average_comprehensiveness_score = sum(normalized_scores) / len(normalized_scores)\n",
        "    print(\"Comprehensiveness Score:\", average_comprehensiveness_score)\n",
        "else:\n",
        "    print(\"No comprehensiveness scores calculated.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-ru5T3AncLX7",
        "outputId": "14dafdb2-f5a6-4588-aaea-1a8e00ea62c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comprehensiveness Score: 0.8256565807221421\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import openai\n",
        "\n",
        "# Set your OpenAI API key\n",
        "openai.api_key = 'sk-proj-9hKjQtT6e6orqiwwd10jT3BlbkFJkW2KlXLZ4Ir2B7s6ja5V'\n",
        "\n",
        "# Load the dataset\n",
        "dataset_path = '/content/Train6.xlsx'  # Update with the path to your dataset\n",
        "df = pd.read_excel(dataset_path)\n",
        "\n",
        "# Define a function to assess the comprehensiveness of a description using ChatGPT\n",
        "def assess_comprehensiveness(disease, description):\n",
        "    try:\n",
        "        # Generate a prompt asking ChatGPT to assess the quality and completeness of the description\n",
        "        prompt = f\"Assess the comprehensiveness of the following description for the disease '{disease}':\\n'{description}'\"\n",
        "\n",
        "        # Use ChatGPT to generate a response\n",
        "        response = openai.Completion.create(\n",
        "            engine=\"gpt-3.5-turbo-instruct\",\n",
        "            prompt=prompt,\n",
        "            max_tokens=100,\n",
        "            temperature=0.3\n",
        "        )\n",
        "\n",
        "        # Extract the completion text from the response\n",
        "        completion_text = response.choices[0].text.strip()\n",
        "\n",
        "        # For simplicity, let's assume the comprehensiveness score is the length of the completion\n",
        "        # You may need to adjust this based on the expected response format\n",
        "        comprehensiveness_score = len(completion_text)\n",
        "\n",
        "        return comprehensiveness_score\n",
        "    except Exception as e:\n",
        "        print(f\"Error assessing comprehensiveness: {e}\")\n",
        "        return None\n",
        "\n",
        "# Iterate over each row in the dataset and assess comprehensiveness\n",
        "comprehensiveness_scores = []\n",
        "for index, row in df.iterrows():\n",
        "    score = assess_comprehensiveness(row['Disease'], row['Description'])\n",
        "    if score is not None:\n",
        "        comprehensiveness_scores.append(score)\n",
        "\n",
        "# Normalize the scores to a scale of 0 to 1\n",
        "max_score = max(comprehensiveness_scores)\n",
        "normalized_scores = [score / max_score for score in comprehensiveness_scores]\n",
        "\n",
        "# Calculate the average comprehensiveness score for the dataset\n",
        "if normalized_scores:\n",
        "    average_comprehensiveness_score = sum(normalized_scores) / len(normalized_scores)\n",
        "    print(\"Comprehensiveness Score:\", average_comprehensiveness_score)\n",
        "else:\n",
        "    print(\"No comprehensiveness scores calculated.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RUUdA4S-cNU_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e8cb06f-c49a-49c5-8591-efe5fd52a409"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comprehensiveness Score: 0.8265200837678565\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import openai\n",
        "\n",
        "# Set your OpenAI API key\n",
        "openai.api_key = 'sk-proj-9hKjQtT6e6orqiwwd10jT3BlbkFJkW2KlXLZ4Ir2B7s6ja5V'\n",
        "\n",
        "# Load the dataset\n",
        "dataset_path = '/content/Train7.xlsx'  # Update with the path to your dataset\n",
        "df = pd.read_excel(dataset_path)\n",
        "\n",
        "# Define a function to assess the comprehensiveness of a description using ChatGPT\n",
        "def assess_comprehensiveness(disease, description):\n",
        "    try:\n",
        "        # Generate a prompt asking ChatGPT to assess the quality and completeness of the description\n",
        "        prompt = f\"Assess the comprehensiveness of the following description for the disease '{disease}':\\n'{description}'\"\n",
        "\n",
        "        # Use ChatGPT to generate a response\n",
        "        response = openai.Completion.create(\n",
        "            engine=\"gpt-3.5-turbo-instruct\",\n",
        "            prompt=prompt,\n",
        "            max_tokens=100,\n",
        "            temperature=0.3\n",
        "        )\n",
        "\n",
        "        # Extract the completion text from the response\n",
        "        completion_text = response.choices[0].text.strip()\n",
        "\n",
        "        # For simplicity, let's assume the comprehensiveness score is the length of the completion\n",
        "        # You may need to adjust this based on the expected response format\n",
        "        comprehensiveness_score = len(completion_text)\n",
        "\n",
        "        return comprehensiveness_score\n",
        "    except Exception as e:\n",
        "        print(f\"Error assessing comprehensiveness: {e}\")\n",
        "        return None\n",
        "\n",
        "# Iterate over each row in the dataset and assess comprehensiveness\n",
        "comprehensiveness_scores = []\n",
        "for index, row in df.iterrows():\n",
        "    score = assess_comprehensiveness(row['Disease'], row['Description'])\n",
        "    if score is not None:\n",
        "        comprehensiveness_scores.append(score)\n",
        "\n",
        "# Normalize the scores to a scale of 0 to 1\n",
        "max_score = max(comprehensiveness_scores)\n",
        "normalized_scores = [score / max_score for score in comprehensiveness_scores]\n",
        "\n",
        "# Calculate the average comprehensiveness score for the dataset\n",
        "if normalized_scores:\n",
        "    average_comprehensiveness_score = sum(normalized_scores) / len(normalized_scores)\n",
        "    print(\"Comprehensiveness Score:\", average_comprehensiveness_score)\n",
        "else:\n",
        "    print(\"No comprehensiveness scores calculated.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YpBDBnBacPnv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a7d5fa8-0a41-4dd2-d9fa-913ec8ac8654"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comprehensiveness Score: 0.8111764503000619\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import openai\n",
        "\n",
        "# Set your OpenAI API key\n",
        "openai.api_key = 'sk-proj-9hKjQtT6e6orqiwwd10jT3BlbkFJkW2KlXLZ4Ir2B7s6ja5V'\n",
        "\n",
        "# Load the dataset\n",
        "dataset_path = '/content/Train8.xlsx'  # Update with the path to your dataset\n",
        "df = pd.read_excel(dataset_path)\n",
        "\n",
        "# Define a function to assess the comprehensiveness of a description using ChatGPT\n",
        "def assess_comprehensiveness(disease, description):\n",
        "    try:\n",
        "        # Generate a prompt asking ChatGPT to assess the quality and completeness of the description\n",
        "        prompt = f\"Assess the comprehensiveness of the following description for the disease '{disease}':\\n'{description}'\"\n",
        "\n",
        "        # Use ChatGPT to generate a response\n",
        "        response = openai.Completion.create(\n",
        "            engine=\"gpt-3.5-turbo-instruct\",\n",
        "            prompt=prompt,\n",
        "            max_tokens=100,\n",
        "            temperature=0.3\n",
        "        )\n",
        "\n",
        "        # Extract the completion text from the response\n",
        "        completion_text = response.choices[0].text.strip()\n",
        "\n",
        "        # For simplicity, let's assume the comprehensiveness score is the length of the completion\n",
        "        # You may need to adjust this based on the expected response format\n",
        "        comprehensiveness_score = len(completion_text)\n",
        "\n",
        "        return comprehensiveness_score\n",
        "    except Exception as e:\n",
        "        print(f\"Error assessing comprehensiveness: {e}\")\n",
        "        return None\n",
        "\n",
        "# Iterate over each row in the dataset and assess comprehensiveness\n",
        "comprehensiveness_scores = []\n",
        "for index, row in df.iterrows():\n",
        "    score = assess_comprehensiveness(row['Disease'], row['Description'])\n",
        "    if score is not None:\n",
        "        comprehensiveness_scores.append(score)\n",
        "\n",
        "# Normalize the scores to a scale of 0 to 1\n",
        "max_score = max(comprehensiveness_scores)\n",
        "normalized_scores = [score / max_score for score in comprehensiveness_scores]\n",
        "\n",
        "# Calculate the average comprehensiveness score for the dataset\n",
        "if normalized_scores:\n",
        "    average_comprehensiveness_score = sum(normalized_scores) / len(normalized_scores)\n",
        "    print(\"Comprehensiveness Score:\", average_comprehensiveness_score)\n",
        "else:\n",
        "    print(\"No comprehensiveness scores calculated.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_scores = [0.8063, 0.8213, 0.8198, 0.8236, 0.8227, 0.837, 0.8257, 0.8265, 0.8112, 0.7994]  # Replace these values with your actual output correctness scores\n",
        "\n",
        "# Calculate the average correctness score\n",
        "avg_comprehensiveness_score = sum(output_scores) / len(output_scores)\n",
        "\n",
        "# Print the average correctness score\n",
        "print(f\"Average Comprehensiveness Score: {round(avg_comprehensiveness_score, 5)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDF3o32fi7x-",
        "outputId": "9dc206c8-64fc-4a3c-e3d9-f6cbea791128"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Comprehensiveness Score: 0.81935\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from tqdm import tqdm  # Import tqdm for progress bar\n",
        "\n",
        "def get_gpt_summary(api_key, completeness_score, is_definition=False):\n",
        "    \"\"\" This function sends a prompt to GPT-3.5 Turbo and gets a summary based on dataset completeness. \"\"\"\n",
        "    url = \"https://api.openai.com/v1/chat/completions\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {api_key}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "    }\n",
        "    if is_definition:\n",
        "        prompt = \"Can you provide a definition of completeness?\"\n",
        "    else:\n",
        "        prompt = f\"The dataset has been analyzed for completeness. The completeness score is {completeness_score:.2f}. Please provide a summary or analysis based on this completeness score.\"\n",
        "    payload = {\n",
        "        \"model\": \"gpt-3.5-turbo\",\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n",
        "    }\n",
        "    response = requests.post(url, headers=headers, json=payload)\n",
        "    if response.status_code == 200:\n",
        "        return response.json()['choices'][0]['message']['content']\n",
        "    else:\n",
        "        raise Exception(f\"API request failed with status {response.status_code}: {response.text}\")\n",
        "\n",
        "def calculate_completeness(df):\n",
        "    \"\"\" This function calculates the completeness of a dataset based on the presence of non-empty 'Description'. \"\"\"\n",
        "    try:\n",
        "        # Count non-empty descriptions\n",
        "        non_empty_count = df['Description'].astype(str).apply(lambda x: len(x.strip()) > 0).sum()\n",
        "        total_count = len(df)\n",
        "        completeness_score = non_empty_count / total_count\n",
        "        return completeness_score\n",
        "    except Exception as e:\n",
        "        raise Exception(f\"Error calculating completeness: {str(e)}\")\n",
        "\n",
        "# List of file paths for datasets\n",
        "file_paths = ['/content/Train0.xlsx', '/content/Train1.xlsx', '/content/Train2.xlsx', '/content/Train3.xlsx', '/content/Train4.xlsx', '/content/Train5.xlsx', '/content/Train6.xlsx', '/content/Train7.xlsx', '/content/Train8.xlsx', '/content/Train9.xlsx']\n",
        "\n",
        "# API key configuration\n",
        "api_key = \"sk-proj-3YCkDmuXm3Vd8LutTzW6T3BlbkFJUkO8khvklysz13dmt5Er\"  # Replace with your actual OpenAI API key\n",
        "\n",
        "# Initialize lists to store completeness scores and summaries\n",
        "completeness_scores = []\n",
        "summaries = []\n",
        "\n",
        "# Loop through each dataset\n",
        "for file_path in file_paths:\n",
        "    print(f\"Processing dataset: {file_path}\")\n",
        "    # Load the dataset\n",
        "    df = pd.read_excel(file_path)\n",
        "\n",
        "    # Calculate the completeness score\n",
        "    with tqdm(total=len(df)) as pbar:\n",
        "        completeness_score = calculate_completeness(df)\n",
        "        completeness_scores.append(completeness_score)\n",
        "        pbar.update(len(df))\n",
        "\n",
        "    # Get the GPT summary\n",
        "    print(\"Fetching GPT summary...\")\n",
        "    with tqdm(total=1) as pbar:\n",
        "        summary = get_gpt_summary(api_key, completeness_score)\n",
        "        summaries.append(summary)\n",
        "        pbar.update(1)\n",
        "\n",
        "# Print completeness scores for all datasets\n",
        "print(\"Completeness Scores:\")\n",
        "for i, score in enumerate(completeness_scores):\n",
        "    print(f\"Dataset {i}: {score:.2f}\")\n",
        "\n",
        "# Print average completeness score\n",
        "avg_completeness_score = sum(completeness_scores) / len(completeness_scores)\n",
        "print(f\"Average Completeness Score: {avg_completeness_score:.2f}\")\n",
        "\n",
        "# Print summaries for all datasets\n",
        "print(\"GPT Summaries on Completeness:\")\n",
        "for i, summary in enumerate(summaries):\n",
        "    print(f\"Dataset {i}:\")\n",
        "    print(summary)\n",
        "    print(\"\\n\")\n",
        "\n",
        "# Print completeness definition from GPT\n",
        "print(\"Completeness Definition from GPT:\")\n",
        "completeness_definition = get_gpt_summary(api_key, 0, is_definition=True)  # Retrieve definition\n",
        "print(completeness_definition)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NFxTsAzE4Eh",
        "outputId": "3e643464-53c2-4758-9ee1-250fb94626bd"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing dataset: /content/Train0.xlsx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4816/4816 [00:00<00:00, 1030232.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching GPT summary...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:02<00:00,  2.21s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing dataset: /content/Train1.xlsx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4817/4817 [00:00<00:00, 765069.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching GPT summary...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:01<00:00,  1.97s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing dataset: /content/Train2.xlsx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4791/4791 [00:00<00:00, 989945.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching GPT summary...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:01<00:00,  1.74s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing dataset: /content/Train3.xlsx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4812/4812 [00:00<00:00, 939705.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching GPT summary...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:01<00:00,  1.64s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing dataset: /content/Train4.xlsx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4811/4811 [00:00<00:00, 1088275.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching GPT summary...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:01<00:00,  1.59s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing dataset: /content/Train5.xlsx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4801/4801 [00:00<00:00, 1264163.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching GPT summary...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:01<00:00,  1.41s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing dataset: /content/Train6.xlsx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4819/4819 [00:00<00:00, 893442.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching GPT summary...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:01<00:00,  1.61s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing dataset: /content/Train7.xlsx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4790/4790 [00:00<00:00, 1063507.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching GPT summary...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:01<00:00,  1.87s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing dataset: /content/Train8.xlsx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4788/4788 [00:00<00:00, 1111362.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching GPT summary...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:02<00:00,  2.01s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing dataset: /content/Train9.xlsx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4812/4812 [00:00<00:00, 1783264.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching GPT summary...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:02<00:00,  2.13s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completeness Scores:\n",
            "Dataset 0: 1.00\n",
            "Dataset 1: 1.00\n",
            "Dataset 2: 1.00\n",
            "Dataset 3: 1.00\n",
            "Dataset 4: 1.00\n",
            "Dataset 5: 1.00\n",
            "Dataset 6: 1.00\n",
            "Dataset 7: 1.00\n",
            "Dataset 8: 1.00\n",
            "Dataset 9: 1.00\n",
            "Average Completeness Score: 1.00\n",
            "GPT Summaries on Completeness:\n",
            "Dataset 0:\n",
            "A completeness score of 1.00 indicates that the dataset has no missing values or incomplete data. This means that all the required fields in the dataset have been filled in and there are no gaps in the data. This high level of completeness makes the dataset highly reliable and trustworthy for analysis and decision-making. Researchers can confidently use this dataset for their studies, as they can be assured that the data is complete and accurate.\n",
            "\n",
            "\n",
            "Dataset 1:\n",
            "A completeness score of 1.00 indicates that all the data points in the dataset are present and there are no missing values. This is a positive sign as it means the dataset is comprehensive and can be relied upon for analysis and decision-making. Researchers can have confidence in the accuracy and reliability of the data, which can lead to more robust findings and conclusions. It is important to note, however, that even with a completeness score of 1.00, it is still essential to assess the quality and relevance of the data to ensure it is suitable for the intended analysis.\n",
            "\n",
            "\n",
            "Dataset 2:\n",
            "A completeness score of 1.00 indicates that the dataset is fully complete, meaning there are no missing values or incomplete entries in the data. This is a positive indication that the dataset is reliable and can be used for analysis with confidence. Researchers can trust that all data points are present and accounted for, leading to more accurate and robust findings. It is important to continue verifying the data quality and ensuring its completeness to maintain the reliability of future analyses.\n",
            "\n",
            "\n",
            "Dataset 3:\n",
            "A completeness score of 1.00 indicates that the dataset is fully complete, meaning that there are no missing values or incomplete data entries. This is important for ensuring the accuracy and reliability of any analysis or findings based on the dataset. Researchers can have confidence in drawing conclusions and making decisions based on the data without concerns about missing or erroneous information. Overall, the high completeness score suggests that the dataset is reliable and ready for further analysis.\n",
            "\n",
            "\n",
            "Dataset 4:\n",
            "A completeness score of 1.00 indicates that there are no missing values or gaps in the dataset. This means that all necessary information is present and accounted for, which increases the reliability and accuracy of any analysis or conclusions drawn from the data. Researchers can have confidence in the data's integrity and use it to make informed decisions or predictions. Additionally, having a complete dataset allows for a more comprehensive analysis and a better understanding of the topic under study.\n",
            "\n",
            "\n",
            "Dataset 5:\n",
            "Based on the completeness score of 1.00, it can be concluded that the dataset is fully complete and there are no missing values or gaps in the data. This indicates that all necessary information is available for analysis and interpretation. This high level of completeness enhances the reliability and accuracy of any conclusions drawn from the dataset. Researchers can confidently use this dataset for further analysis and decision-making processes, knowing that they have access to all the required information.\n",
            "\n",
            "\n",
            "Dataset 6:\n",
            "A completeness score of 1.00 indicates that all the data points in the dataset are present and there are no missing values. This suggests that the dataset is very thorough and reliable for analysis. Researchers can confidently use this dataset for various analyses and draw meaningful conclusions from the data. It is important to note that while the completeness score is high, it is also important to consider the quality and accuracy of the data to ensure the validity of the results.\n",
            "\n",
            "\n",
            "Dataset 7:\n",
            "Based on the completeness score of 1.00, it appears that all the data points in the dataset have values for all the variables. This indicates that there are no missing values in the dataset, and all the information that was intended to be collected was successfully captured. The high completeness score suggests that the dataset is reliable for analysis and can provide accurate insights into the phenomenon being studied. Researchers can confidently use this dataset for further analysis and draw meaningful conclusions from the data.\n",
            "\n",
            "\n",
            "Dataset 8:\n",
            "Based on a completeness score of 1.00, it indicates that the dataset is considered complete, meaning that there are no missing values or significant gaps in the data. This is a positive indicator as it suggests that the dataset is reliable and can be used confidently for further analysis or decision-making. Researchers can have confidence in the accuracy and comprehensiveness of the information provided in the dataset. This high completeness score also implies that there is a low risk of bias or errors in the data, allowing for robust and accurate analyses to be conducted. Overall, a completeness score of 1.00 demonstrates the quality of the dataset and enhances its suitability for a wide range of analytical purposes.\n",
            "\n",
            "\n",
            "Dataset 9:\n",
            "Based on the completeness score of 1.00, it can be concluded that the dataset is complete with no missing values or gaps in the data. This means that all the necessary variables and information have been successfully collected and documented, making the dataset reliable for analysis and interpretation. Researchers and analysts can confidently proceed with their analysis knowing that there are no missing pieces of information that could potentially skew the results. This high level of completeness enhances the overall quality and reliability of the dataset, allowing for accurate insights and conclusions to be drawn from the data.\n",
            "\n",
            "\n",
            "Completeness Definition from GPT:\n",
            "Completeness is the state of being whole or having all necessary parts or elements. It can also refer to a sense of being finished or having achieved perfection or satisfaction. In mathematics, completeness refers to a property of certain sets or spaces that contain all their limit points.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNbPkndbqxp74lM1uUWZLUx",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}