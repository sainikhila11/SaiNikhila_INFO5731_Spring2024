{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sainikhila11/SaiNikhila_INFO5731_Spring2024/blob/main/Yavanamanda_Sai_Assignment_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ryk8D1Q4Wsrp"
      },
      "source": [
        "# **INFO5731 Assignment 2**\n",
        "\n",
        "In this assignment, you will work on gathering text data from an open data source via web scraping or API. Following this, you will need to clean the text data and perform syntactic analysis on the data. Follow the instructions carefully and design well-structured Python programs to address each question.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "* **Make sure to submit the cleaned data CSV in the comment section - 10 points**\n",
        "\n",
        "**Total points**: 100\n",
        "\n",
        "**Deadline**: Wednesday, at 11:59 PM.\n",
        "\n",
        "**Late Submission will have a penalty of 10% reduction for each day after the deadline.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkzR8cFAyGik"
      },
      "source": [
        "# Question 1 (40 points)\n",
        "\n",
        "Write a python program to collect text data from **either of the following sources** and save the data into a **csv file:**\n",
        "\n",
        "(1) Collect all the customer reviews of a product (you can choose any porduct) on amazon. [atleast 1000 reviews]\n",
        "\n",
        "(2) Collect the top 1000 User Reviews of a movie recently in 2023 or 2024 (you can choose any movie) from IMDB. [If one movie doesn't have sufficient reviews, collect reviews of atleast 2 or 3 movies]\n",
        "\n",
        "(3) Collect all the reviews of the top 1000 most popular software from G2 or Capterra.\n",
        "\n",
        "(4) Collect the **abstracts** of the top 10000 research papers by using the query \"machine learning\", \"data science\", \"artifical intelligence\", or \"information extraction\" from Semantic Scholar.\n",
        "\n",
        "(5) Collect all the information of the 904 narrators in the Densho Digital Repository.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "jDyTKYs-yGit",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "395e1eeb-33de-4f6a-c9de-9f4f55369023"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully saved 1000 reviews with titles.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_04b55c64-4d27-4515-b45a-2802cd66d5f1\", \"imdb_reviews.csv\", 1314774)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import time\n",
        "\n",
        "def movie_reviews(movie_id):\n",
        "  #url for reviews page of the movie 'Barbie'\n",
        "    url = f'https://www.imdb.com/title/tt1517268/reviews'\n",
        "    #create list for reviews\n",
        "    reviews = []\n",
        "    try:\n",
        "        while True:\n",
        "          # request.get takes the request to the specified url and retreives to response\n",
        "            response = requests.get(url)\n",
        "            # extract html content from http response\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "            # extract div elements from 'review contaainer' class\n",
        "            review_containers = soup.find_all('div', class_='review-container')\n",
        "\n",
        "            for container in review_containers:\n",
        "                # Find the title within the container. this gives us the short summarized review of the text.\n",
        "                title_element = container.find('a', class_='title')\n",
        "                title = title_element.get_text(strip=True) if title_element else 'No Title'\n",
        "\n",
        "                # Find the review text within the container. this gives us the complete review posted by user usng the text show-more__control\n",
        "                review_text_element = container.find('div', class_='text show-more__control')\n",
        "                review_text = review_text_element.get_text(strip=True) if review_text_element else 'No Review Text'\n",
        "\n",
        "                # add retreived data to reviews list\n",
        "                reviews.append((title, review_text))\n",
        "\n",
        "            # conditional loop to retrieve reviews as the count we need is 1000\n",
        "            if len(reviews) >= 1000 or not review_containers:\n",
        "                break\n",
        "\n",
        "            time.sleep(1)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f'Error occurred: {e}')\n",
        "\n",
        "    return reviews[:1000]\n",
        "\n",
        "movie_ids = ['tt1517268']  #id in imdb for barbie movie\n",
        "all_reviews = [] #list to have reviews if multiple movies were taken\n",
        "\n",
        "for movie_id in movie_ids:\n",
        "    all_reviews.extend(movie_reviews(movie_id))\n",
        "    if len(all_reviews) >= 1000:\n",
        "        break\n",
        "\n",
        "# Save the reviews to a CSV file\n",
        "with open('imdb_reviews.csv', 'w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(['Title', 'Review'])  # Header\n",
        "    for title, review in all_reviews:\n",
        "        writer.writerow([title, review])\n",
        "\n",
        "# print number of reviews retrieved\n",
        "print(f'Successfully saved {len(all_reviews)} reviews with titles.')\n",
        "\n",
        "#download the csv file with reviews retrieved to local system\n",
        "from google.colab import files\n",
        "files.download('imdb_reviews.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90_NR8c5XGWc"
      },
      "source": [
        "# Question 2 (30 points)\n",
        "\n",
        "Write a python program to **clean the text data** you collected in the previous question and save the clean data in a new column in the csv file. The data cleaning steps include: [Code and output is required for each part]\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the stopwords list.\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming.\n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "5QX6bJjGWXY9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "c23a2537-4fce-4362-f856-813416a85e18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully saved 1000 cleaned reviews with titles in cleaned_imdb_reviews.csv.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_c3fb94b6-b7d4-4670-b3f7-f1193479b3c8\", \"cleaned_imdb_reviews.csv\", 1269230)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import csv\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def clean_text(text):\n",
        "    # Remove noise (special characters and punctuations)\n",
        "    # all characters are read one after one and only alphanumerics and spaces will be considered rest will be eliminated.\n",
        "    cleaned_text = ''.join(char for char in text if char.isalnum() or char.isspace())\n",
        "\n",
        "    # Remove numbers\n",
        "    # all characters that are not numbers will be considered and moved to cleaned_text variable\n",
        "    cleaned_text = ''.join(char for char in cleaned_text if not char.isdigit())\n",
        "\n",
        "    # Lowercase all texts\n",
        "    cleaned_text = cleaned_text.lower()\n",
        "\n",
        "    return cleaned_text\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    # load stop words and tokenize by using split function\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = text.split()\n",
        "    # remove all stop words and move rest of the text to filtered_text\n",
        "    filtered_text = ' '.join(word for word in words if word.lower() not in stop_words)\n",
        "\n",
        "    return filtered_text\n",
        "\n",
        "def stem_text(text):\n",
        "  # Create an instance for Porter Stemmer algorithm\n",
        "    ps = PorterStemmer()\n",
        "    stemmed_text = ' '.join(ps.stem(word) for word in text.split())\n",
        "\n",
        "    return stemmed_text\n",
        "\n",
        "def lemmatize_text(text):\n",
        "  # Create an instance of the WordNet Lemmatizer\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_text = ' '.join(lemmatizer.lemmatize(word) for word in text.split())\n",
        "\n",
        "    return lemmatized_text\n",
        "\n",
        "# Load the existing CSV file with scraped data\n",
        "input_file_path = 'imdb_reviews.csv'\n",
        "output_file_path = 'cleaned_imdb_reviews.csv'\n",
        "\n",
        "# Read the existing reviews from the CSV file\n",
        "all_reviews = []\n",
        "\n",
        "with open(input_file_path, 'r', encoding='utf-8') as file:\n",
        "    reader = csv.reader(file)\n",
        "    header = next(reader)  # Skip header\n",
        "    for row in reader:\n",
        "        title, review = row\n",
        "        all_reviews.append((title, review))\n",
        "\n",
        "# Clean the reviews\n",
        "cleaned_reviews = []\n",
        "\n",
        "for title, review in all_reviews:\n",
        "    cleaned_title = clean_text(title)\n",
        "    cleaned_review = clean_text(review)\n",
        "\n",
        "\n",
        "    cleaned_reviews.append((cleaned_title, cleaned_review))\n",
        "\n",
        "# Save the cleaned reviews to a new CSV file\n",
        "with open(output_file_path, 'w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(['Cleaned Title', 'Cleaned Review'])  # Header\n",
        "    for cleaned_title, cleaned_review in cleaned_reviews:\n",
        "        writer.writerow([cleaned_title, cleaned_review])\n",
        "\n",
        "# print number of reviews that have been cleaned\n",
        "print(f'Successfully saved {len(cleaned_reviews)} cleaned reviews with titles in {output_file_path}.')\n",
        "\n",
        "#download the csv file with reviews retrieved to local system\n",
        "from google.colab import files\n",
        "files.download('cleaned_imdb_reviews.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F_PZdH9Sh49"
      },
      "source": [
        "# Question 3 (30 points)\n",
        "\n",
        "Write a python program to **conduct syntax and structure analysis of the clean text** you just saved above. The syntax and structure analysis includes:\n",
        "\n",
        "(1) **Parts of Speech (POS) Tagging:** Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) **Constituency Parsing and Dependency Parsing:** print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) **Named Entity Recognition:** Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Y0oOSlsOS0cq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f56741e9-ccf1-4f38-9e02-dcf863933f36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS Tags Count: {'Noun': 30, 'Verb': 21, 'Adjective': 16, 'Adverb': 7}\n",
            "\n",
            "Constituency Parsing Tree:\n",
            "margot <--nsubj-- does\n",
            "does <--ROOT-- does\n",
            "the <--det-- best\n",
            "best <--dobj-- does\n",
            "with <--prep-- best\n",
            "what <--dobj-- given\n",
            "she <--nsubjpass-- given\n",
            "s <--auxpass-- given\n",
            "given <--pcomp-- with\n",
            "but <--cc-- does\n",
            "this <--det-- film\n",
            "film <--nsubj-- was\n",
            "was <--conj-- does\n",
            "very <--advmod-- disappointing\n",
            "disappointing <--acomp-- was\n",
            "to <--prep-- disappointing\n",
            "me <--pobj-- to\n",
            "it <--nsubjpass-- marketed\n",
            "was <--auxpass-- marketed\n",
            "marketed <--ccomp-- was\n",
            "as <--prep-- marketed\n",
            "a <--det-- satire\n",
            "fun <--amod-- satire\n",
            "quirky <--amod-- satire\n",
            "satire <--pobj-- as\n",
            "with <--prep-- satire\n",
            "homages <--pobj-- with\n",
            "to <--prep-- marketed\n",
            "other <--amod-- movies\n",
            "movies <--pobj-- to\n",
            "it <--nsubj-- started\n",
            "started <--relcl-- movies\n",
            "that <--det-- way\n",
            "way <--npadvmod-- started\n",
            "but <--cc-- marketed\n",
            "ended <--conj-- marketed\n",
            "with <--prep-- ended\n",
            "overdramatized <--amod-- speeches\n",
            "speeches <--pobj-- with\n",
            "and <--cc-- speeches\n",
            "an <--det-- ending\n",
            "ending <--conj-- speeches\n",
            "that <--nsubj-- tried\n",
            "clearly <--advmod-- tried\n",
            "tried <--relcl-- ending\n",
            "to <--aux-- make\n",
            "make <--xcomp-- tried\n",
            "the <--det-- audience\n",
            "audience <--nsubj-- feel\n",
            "feel <--ccomp-- make\n",
            "something <--dobj-- feel\n",
            "but <--cc-- ended\n",
            "left <--conj-- ended\n",
            "everyone <--dobj-- left\n",
            "just <--advmod-- feeling\n",
            "feeling <--advcl-- left\n",
            "confused <--acomp-- feeling\n",
            "and <--cc-- feeling\n",
            "before <--mark-- say\n",
            "you <--nsubj-- say\n",
            "say <--advcl-- left\n",
            "i <--nsubj-- m\n",
            "m <--ccomp-- say\n",
            "a <--det-- man\n",
            "crotchety <--amod-- man\n",
            "old <--amod-- man\n",
            "man <--attr-- m\n",
            "i <--nsubj-- m\n",
            "m <--relcl-- man\n",
            "a <--det-- woman\n",
            "woman <--attr-- m\n",
            "in <--prep-- woman\n",
            "my <--poss-- s\n",
            "s <--pobj-- in\n",
            "so <--advmod-- m\n",
            "i <--nsubj-- m\n",
            "m <--advcl-- is\n",
            "pretty <--advmod-- sure\n",
            "sure <--acomp-- m\n",
            "i <--nsubj-- m\n",
            "m <--ccomp-- sure\n",
            "this <--det-- movies\n",
            "movies <--attr-- m\n",
            "target <--compound-- audience\n",
            "audience <--attr-- m\n",
            "the <--det-- part\n",
            "saddest <--amod-- part\n",
            "part <--nsubj-- is\n",
            "is <--ROOT-- is\n",
            "there <--expl-- were\n",
            "were <--ccomp-- is\n",
            "parents <--attr-- were\n",
            "with <--prep-- parents\n",
            "their <--poss-- kids\n",
            "kids <--pobj-- with\n",
            "in <--prep-- kids\n",
            "the <--det-- theater\n",
            "theater <--pobj-- in\n",
            "that <--nsubj-- were\n",
            "were <--relcl-- parents\n",
            "victims <--attr-- were\n",
            "of <--prep-- victims\n",
            "the <--det-- marketing\n",
            "poor <--amod-- marketing\n",
            "marketing <--pobj-- of\n",
            "because <--mark-- is\n",
            "this <--nsubj-- is\n",
            "is <--advcl-- were\n",
            "not <--neg-- is\n",
            "a <--det-- movie\n",
            "kids <--compound-- movie\n",
            "movie <--attr-- is\n",
            "overall <--advmod-- was\n",
            "the <--det-- humor\n",
            "humor <--nsubj-- was\n",
            "was <--advcl-- is\n",
            "fun <--acomp-- was\n",
            "on <--prep-- was\n",
            "occasion <--pobj-- on\n",
            "and <--cc-- was\n",
            "the <--det-- film\n",
            "film <--nsubj-- is\n",
            "is <--conj-- was\n",
            "beautiful <--acomp-- is\n",
            "to <--aux-- look\n",
            "look <--xcomp-- is\n",
            "at <--prep-- look\n",
            "but <--cc-- is\n",
            "the <--det-- concept\n",
            "whole <--amod-- concept\n",
            "concept <--nsubj-- falls\n",
            "falls <--conj-- is\n",
            "apart <--advmod-- falls\n",
            "in <--prep-- falls\n",
            "the <--det-- half\n",
            "second <--amod-- half\n",
            "half <--pobj-- in\n",
            "of <--prep-- half\n",
            "the <--det-- film\n",
            "film <--pobj-- of\n",
            "and <--cc-- falls\n",
            "becomes <--conj-- falls\n",
            "a <--det-- party\n",
            "pity <--compound-- party\n",
            "party <--attr-- becomes\n",
            "for <--prep-- party\n",
            "the <--det-- woman\n",
            "strong <--amod-- woman\n",
            "woman <--pobj-- for\n",
            "\n",
            "Named Entity Recognition (NER) Count:\n",
            "PERSON: 0\n",
            "ORG: 0\n",
            "LOC: 0\n",
            "PRODUCT: 0\n",
            "DATE: 1\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Read the cleaned reviews from the CSV file\n",
        "cleaned_reviews = []\n",
        "\n",
        "with open('cleaned_imdb_reviews.csv', 'r', encoding='utf-8') as file:\n",
        "    reader = csv.reader(file)\n",
        "    header = next(reader)  # Skip header\n",
        "    for row in reader:\n",
        "        cleaned_title, cleaned_review = row\n",
        "        cleaned_reviews.append((cleaned_title, cleaned_review))\n",
        "\n",
        "# Choose a sample review for analysis\n",
        "sample_review = cleaned_reviews[0][1]  # Choose the review text from the first row\n",
        "\n",
        "# (1) Parts of Speech (POS) Tagging\n",
        "doc = nlp(sample_review)\n",
        "pos_tags_count = {'Noun': 0, 'Verb': 0, 'Adjective': 0, 'Adverb': 0}\n",
        "\n",
        "for token in doc:\n",
        "    if token.pos_ == 'NOUN':\n",
        "        pos_tags_count['Noun'] += 1\n",
        "    elif token.pos_ == 'VERB':\n",
        "        pos_tags_count['Verb'] += 1\n",
        "    elif token.pos_ == 'ADJ':\n",
        "        pos_tags_count['Adjective'] += 1\n",
        "    elif token.pos_ == 'ADV':\n",
        "        pos_tags_count['Adverb'] += 1\n",
        "\n",
        "print('POS Tags Count:', pos_tags_count)\n",
        "\n",
        "# (2) Constituency Parsing and Dependency Parsing\n",
        "# Print constituency parsing tree\n",
        "print('\\nConstituency Parsing Tree:')\n",
        "for sent in doc.sents:\n",
        "    for token in sent:\n",
        "        print(f\"{token.text} <--{token.dep_}-- {token.head.text}\")\n",
        "\n",
        "# (3) Named Entity Recognition (NER)\n",
        "entities_count = {'PERSON': 0, 'ORG': 0, 'LOC': 0, 'PRODUCT': 0, 'DATE': 0}\n",
        "\n",
        "for ent in doc.ents:\n",
        "    entities_count[ent.label_] += 1\n",
        "\n",
        "print('\\nNamed Entity Recognition (NER) Count:')\n",
        "for entity, count in entities_count.items():\n",
        "    print(f\"{entity}: {count}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question\n",
        "\n",
        "Provide your thoughts on the assignment. What did you find challenging, and what aspects did you enjoy? Your opinion on the provided time to complete the assignment."
      ],
      "metadata": {
        "id": "q8BFCvWp32cf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This assignemnt has been a little more helpful in terms of understanding web scraping with python better than the previous assignment. The previous assignment did work like an introduction to web scrape using python whereas this assignment helped in applying that to code. learning new libraries and functions that python provide to scrape and clean data has been very intresting.I did find a little trouble in giving the right input and right url to scrape in the question one, but looking through the HTML page source gave me the idea on which tags is having the data that i need. for example the 'div' tags has the fully written film revies under class 'text show-more__control' and the 'a' tags have the short version of reviews in class 'title'. Exploring through the source page to find the right elemts to tag in the code to perform scraping was something new that i had worked on. I do think the time provided is good enough for the assignment."
      ],
      "metadata": {
        "id": "dklUkkmmOb0e"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "OP4-5qxAtInv"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}