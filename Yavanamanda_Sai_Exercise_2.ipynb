{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sainikhila11/SaiNikhila_INFO5731_Spring2024/blob/main/Yavanamanda_Sai_Exercise_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DymRJbxDBCnf"
      },
      "source": [
        "# **INFO5731 In-class Exercise 2**\n",
        "\n",
        "The purpose of this exercise is to understand users' information needs, and then collect data from different sources for analysis by implementing web scraping using Python.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting research question (or practical question or something innovative) you have in mind, what kind of data should be collected to answer the question(s)? Specify the amount of data needed for analysis. Provide detailed steps for collecting and saving the data."
      ],
      "metadata": {
        "id": "FBKvD6O_TY6e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The topic i am intrested to do on for this assesssment is how do daily variations in social media engagement relate to fluctuations in individuals' reported happiness levels? To answer this question, additional data on the specific content or interactions within each social media platform,such as the nature of posts or the sentiment of interactions, would be crucial. Furthermore, incorporating external factors like daily events, news sentiment, or weather conditions could enhance the analysis. A substantial dataset, with at least a year's worth of daily observations for a diverse sample, would be needed to capture seasonal and long-term trends adequately. The steps for collecting and saving the data would involve expanding the social media usage simulation to include more detailed information and merging it with an enriched individual dataset. The resulting dataset could then be saved in a CSV file for further analysis."
      ],
      "metadata": {
        "id": "IItwvJ5pB4yI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2 (10 Points)\n",
        "Write Python code to collect a dataset of 1000 samples related to the question discussed in Question 1."
      ],
      "metadata": {
        "id": "E9RqrlwdTfvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "import random\n",
        "\n",
        "# Simulate social media usage data\n",
        "def generate_social_media_data(start_date, end_date):\n",
        "    date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n",
        "    social_media_data = {'Date': date_range,\n",
        "                         'Facebook_Hours': [random.uniform(0, 3) for _ in range(len(date_range))],\n",
        "                         'Instagram_Hours': [random.uniform(0, 2) for _ in range(len(date_range))],\n",
        "                         'Twitter_Hours': [random.uniform(0, 1) for _ in range(len(date_range))],\n",
        "                         'Other_Hours': [random.uniform(0, 2) for _ in range(len(date_range))]}\n",
        "    return pd.DataFrame(social_media_data)\n",
        "\n",
        "# Simulate individual data\n",
        "def generate_individual_data(start_date, end_date):\n",
        "    date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n",
        "    individual_data = {'Date': date_range,\n",
        "                       'Happiness_Score': [random.randint(1, 10) for _ in range(len(date_range))],\n",
        "                       'Age': [random.randint(18, 60) for _ in range(len(date_range))],\n",
        "                       'Gender': [random.choice(['Male', 'Female']) for _ in range(len(date_range))]}\n",
        "    return pd.DataFrame(individual_data)\n",
        "\n",
        "# Set the timeframe for data collection\n",
        "start_date = datetime(2023, 1, 1)\n",
        "end_date = datetime(2023, 3, 31)\n",
        "\n",
        "# Generate social media usage data\n",
        "social_media_data = generate_social_media_data(start_date, end_date)\n",
        "\n",
        "# Generate individual data\n",
        "individual_data = generate_individual_data(start_date, end_date)\n",
        "\n",
        "# Merge dataframes on 'Date'\n",
        "dataset = pd.merge(social_media_data, individual_data, on='Date')\n",
        "\n",
        "# Save the dataset to a CSV file\n",
        "dataset.to_csv('social_media_happiness_dataset.csv', index=False)\n",
        "\n",
        "# Load the dataset\n",
        "loaded_dataset = pd.read_csv('social_media_happiness_dataset.csv')\n",
        "\n",
        "# Display the first few rows of the loaded dataset\n",
        "print(\"First few rows of the loaded dataset:\")\n",
        "print(loaded_dataset.head())\n",
        "\n",
        "# Display basic statistics of the loaded dataset\n",
        "print(\"\\nBasic statistics of the loaded dataset:\")\n",
        "print(loaded_dataset.describe())\n",
        "\n",
        "# Display the unique values in the 'Gender' column\n",
        "print(\"\\nUnique values in the 'Gender' column:\")\n",
        "print(loaded_dataset['Gender'].unique())\n"
      ],
      "metadata": {
        "id": "4XvRknixTh1g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76bb150b-0fcb-4f1c-c9df-3f2b4cd08bd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First few rows of the loaded dataset:\n",
            "         Date  Facebook_Hours  Instagram_Hours  Twitter_Hours  Other_Hours  \\\n",
            "0  2023-01-01        1.904099         0.443164       0.997229     0.084667   \n",
            "1  2023-01-02        1.007663         0.361478       0.737084     0.597787   \n",
            "2  2023-01-03        0.977409         1.109788       0.801282     0.557709   \n",
            "3  2023-01-04        0.256258         1.304992       0.397953     1.665752   \n",
            "4  2023-01-05        0.067369         0.139420       0.753975     1.832711   \n",
            "\n",
            "   Happiness_Score  Age  Gender  \n",
            "0                6   40    Male  \n",
            "1                5   20  Female  \n",
            "2                5   39  Female  \n",
            "3                6   26    Male  \n",
            "4                2   19  Female  \n",
            "\n",
            "Basic statistics of the loaded dataset:\n",
            "       Facebook_Hours  Instagram_Hours  Twitter_Hours  Other_Hours  \\\n",
            "count       90.000000        90.000000      90.000000    90.000000   \n",
            "mean         1.427203         0.976420       0.513907     0.938930   \n",
            "std          0.860750         0.566659       0.285841     0.560737   \n",
            "min          0.011290         0.019550       0.011577     0.010979   \n",
            "25%          0.691087         0.417503       0.296916     0.516196   \n",
            "50%          1.510055         1.039500       0.470355     0.811760   \n",
            "75%          2.065080         1.478246       0.789009     1.298817   \n",
            "max          2.931619         1.987391       0.997229     1.995309   \n",
            "\n",
            "       Happiness_Score        Age  \n",
            "count        90.000000  90.000000  \n",
            "mean          5.655556  36.844444  \n",
            "std           2.797114  12.575088  \n",
            "min           1.000000  18.000000  \n",
            "25%           3.000000  26.250000  \n",
            "50%           6.000000  36.000000  \n",
            "75%           8.000000  47.750000  \n",
            "max          10.000000  60.000000  \n",
            "\n",
            "Unique values in the 'Gender' column:\n",
            "['Male' 'Female']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03jb4GZsBkBS"
      },
      "source": [
        "## Question 3 (10 Points)\n",
        "Write Python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"XYZ\". The articles should be published in the last 10 years (2014-2024).\n",
        "\n",
        "The following information from the article needs to be collected:\n",
        "\n",
        "(1) Title of the article\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "YaGLbSHHB8Ej"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def fetch_google_scholar_articles(keyword, num_articles=10, years_range=(2014, 2024)):\n",
        "    base_url = \"https://scholar.google.com/\"\n",
        "    articles = []\n",
        "\n",
        "    for year in range(years_range[0], years_range[1] + 1):\n",
        "        params = {\n",
        "            \"q\": f'{keyword} after:{year-1}-12-31 before:{year+1}-01-01',\n",
        "            \"hl\": \"en\",\n",
        "            \"as_sdt\": \"0,5\",\n",
        "        }\n",
        "\n",
        "        response = requests.get(base_url, params=params)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "            results = soup.find_all('div', class_='gs_ri')\n",
        "\n",
        "            for result in results:\n",
        "                title_element = result.find('h3', class_='gs_rt')\n",
        "                title = title_element.get_text(strip=True) if title_element else \"Title not found\"\n",
        "\n",
        "                venue = result.find('div', class_='gs_a').get_text(strip=True) if result.find('div', class_='gs_a') else \"Venue not found\"\n",
        "                year_match = [int(y) for y in re.findall(r'\\b\\d{4}\\b', venue) if years_range[0] <= int(y) <= years_range[1]]\n",
        "                year = year_match[0] if year_match else None\n",
        "                authors = venue.split('-')[0].strip()\n",
        "\n",
        "                abstract_element = result.find('div', class_='gs_rs')\n",
        "                abstract = abstract_element.get_text(strip=True) if abstract_element else \"Abstract not found\"\n",
        "\n",
        "                article_info = {\n",
        "                    \"title\": title,\n",
        "                    \"venue\": venue,\n",
        "                    \"year\": year,\n",
        "                    \"authors\": authors,\n",
        "                    \"abstract\": abstract,\n",
        "                }\n",
        "\n",
        "                articles.append(article_info)\n",
        "\n",
        "    return articles\n",
        "\n",
        "# Example usage\n",
        "keyword = \"XYZ\"\n",
        "num_articles = 10\n",
        "articles = fetch_google_scholar_articles(keyword, num_articles)\n",
        "\n",
        "for i, article in enumerate(articles, 1):\n",
        "    print(f\"Article {i}:\")\n",
        "    print(f\"Title: {article['title']}\")\n",
        "    print(f\"Venue: {article['venue']}\")\n",
        "    print(f\"Year: {article['year']}\")\n",
        "    print(f\"Authors: {article['authors']}\")\n",
        "    print(f\"Abstract: {article['abstract']}\")\n",
        "    print(\"=\" * 50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJDe71iLB616"
      },
      "source": [
        "## Question 4A (10 Points)\n",
        "Develop Python code to collect data from social media platforms like Reddit, Instagram, Twitter (formerly known as X), Facebook, or any other. Use hashtags, keywords, usernames, or user IDs to gather the data.\n",
        "\n",
        "\n",
        "\n",
        "Ensure that the collected data has more than four columns.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55W9AMdXCSpV"
      },
      "source": [
        "## Question 4B (10 Points)\n",
        "If you encounter challenges with Question-4 web scraping using Python, employ any online tools such as ParseHub or Octoparse for data extraction. Introduce the selected tool, outline the steps for web scraping, and showcase the final output in formats like CSV or Excel.\n",
        "\n",
        "\n",
        "\n",
        "Upload a document (Word or PDF File) in any shared storage (preferably UNT OneDrive) and add the publicly accessible link in the below code cell.\n",
        "\n",
        "Please only choose one option for question 4. If you do both options, we will grade only the first one"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used the online tool 'web scraper' to do the web scraping. I had chosen the platform linkedIn to collect job details.I Have installed the webscraper extention to my local system and initiated it in the page i want to extract the data from. I have moved to web scraper tab and started creating a sitemap by giving a name to it and URL from where i need to scrape the data. I have then created a root node that consisted all the sub elements. For each of these i had given a name and then given their type such as link, text, url etc and then selected the data that has to go to each of the elements and clicked on srape sitemap. Then each link on the webpage was scraped and extracted in table format. Then i had selected to export the data as a csv file into the local system.\n",
        "\n",
        "here is the link for drive with the csv file of scraped data\n",
        " https://docs.google.com/spreadsheets/d/1JVjLmapQ4NBfLnDKxLs6UxCbn-Wd2y1P/edit?usp=drive_link&ouid=106647022315744601568&rtpof=true&sd=true"
      ],
      "metadata": {
        "id": "CgIHH4ahBcpn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "sZOhks1dXWEe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on Web Scraping and Data Collection**\n",
        "\n",
        "\n",
        "\n",
        "Please share your thoughts and feedback on the web scraping and data collection exercises you have completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on web scraping tasks. What were the key concepts or techniques you found most beneficial in understanding the process of extracting data from various online sources?\n",
        "\n",
        "\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in collecting data from certain websites, and how did you overcome them? If you opted for the non-coding option, share your experience with the chosen tool.\n",
        "\n",
        "\n",
        "\n",
        "Relevance to Your Field of Study: How might the ability to gather and analyze data from online sources enhance your work or research?\n",
        "\n",
        "**(no grading of your submission if this question is left unanswered)**"
      ],
      "metadata": {
        "id": "eqmHVEwaWhbV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Web Scraping as a topic definitely stood out as intresting to me and doing this using an online tool hands on gave me practial knowledge on how exactly these tools work to do the scraping. However i found it difficult to code in python to do the scraping. I wasn't very aware of what module and funtions or methods would be suitable for installing the webscraping tools. I think gaining the ability to build more skill in this topic would be extremely helpful given that it makes data extraction and gathering way more easier than manual methods."
      ],
      "metadata": {
        "id": "ZR7ciWXzBYg2"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "FBKvD6O_TY6e",
        "E9RqrlwdTfvl",
        "03jb4GZsBkBS",
        "jJDe71iLB616",
        "55W9AMdXCSpV",
        "4ulBZ6yhCi9F",
        "6SmvS7nSfbj8",
        "sZOhks1dXWEe"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}